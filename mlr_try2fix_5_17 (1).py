# -*- coding: utf-8 -*-
"""MLR_try2fix_5.17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMEy3j2n2K2rkWONcMz_l9uC8nru-ZKz
"""

# -*- coding: utf-8 -*-
"""MLR_try2fix_5.17_updated_full.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/YOUR_DRIVE_LINK_HERE
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
# from sklearn.preprocessing import LabelEncoder # Not actively used for VIF/OLS logic
import matplotlib.pyplot as plt # For plotting
import seaborn as sns # For plotting
from tqdm import tqdm # For progress bars
from google.colab import files # Specific to Google Colab

# Initialize the dictionary to store results from each bucket
all_results = {}

# Step 1: Upload the file
# If running locally, replace this with:
# df_original = pd.read_excel("your_file_path.xlsx")
print("Please upload your Excel file.")
uploaded = files.upload()
excel_file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {excel_file_name}")
df_original = pd.read_excel(excel_file_name)
df = df_original.copy() # Work on a copy

# Step 2: Basic preprocessing
df = df.rename(columns={
    'categoryid': 'categoryId',
    'Time_group': 'Time_Group' # Ensure consistency if this is the actual column name
})

# Filter out unwanted data
# Ensure 'view_count' and 'categoryId' columns exist
if 'categoryId' not in df.columns or 'view_count' not in df.columns:
    raise ValueError("Required columns 'categoryId' or 'view_count' not found in the DataFrame.")

df = df[(df['categoryId'].astype(str) != '24') & (df['view_count'] > 0)]

# Convert columns to appropriate types for dummy encoding and processing
if 'categoryId' in df.columns:
    df['categoryId'] = df['categoryId'].astype(str)

required_cols_for_dummies = ['Time_Group', 'region', 'bucket_label']
for col_name in required_cols_for_dummies:
    if col_name not in df.columns:
        raise ValueError(f"Required column '{col_name}' not found. Please check the column names in your Excel file.")

df['Time_Group'] = df['Time_Group'].astype(str)
df['region'] = df['region'].astype(str)
try:
    df['bucket_label'] = df['bucket_label'].astype(int)
except ValueError as e:
    raise ValueError(f"Error converting 'bucket_label' to int. Please check its values. Original error: {e}")


# Step 3 (Implicit): Define baseline levels if needed for interpretation later
# This depends on how pd.get_dummies with drop_first=True behaves.
# The first category alphabetically/numerically for Time_Group and region will be the baseline.
# You might want to record these for each bucket if they vary and are important for interpretation.


# Step 4: Loop by bucket_label (e.g., 1 to 4)
unique_buckets_in_data = sorted(df['bucket_label'].unique())
# Processing a fixed list as per your original code, but good to know what's in the data.
# buckets_to_process = [1, 2, 3, 4]
buckets_to_process = unique_buckets_in_data # Process all unique buckets found
print(f"Found unique buckets: {unique_buckets_in_data}. Processing: {buckets_to_process}")


for b in buckets_to_process:
    print(f"\n--- Processing Bucket {b} ---")
    df_b = df[df['bucket_label'] == b].copy()
    print(f"Samples in Bucket {b}: {df_b.shape[0]}")

    if df_b.empty:
        print(f"Skipping Bucket {b} due to no data.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - no data"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - no data"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - no data"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - no data"
        continue

    # Get actual unique values for Time_Group and region within the current bucket
    # This helps determine if dummy creation is meaningful
    unique_time_groups_in_bucket = df_b['Time_Group'].nunique()
    unique_regions_in_bucket = df_b['region'].nunique()

    # Step 5: One-hot encode Time_Group and region
    # Only create dummies if there's more than one category to avoid issues with drop_first=True
    # or creating an empty dummies dataframe.
    cols_for_dummies = []
    if unique_time_groups_in_bucket > 1:
        cols_for_dummies.append('Time_Group')
    else:
        print(f"Bucket {b}: 'Time_Group' has only {unique_time_groups_in_bucket} unique value(s). It will not generate dummy variables with drop_first=True.")

    if unique_regions_in_bucket > 1:
        cols_for_dummies.append('region')
    else:
        print(f"Bucket {b}: 'region' has only {unique_regions_in_bucket} unique value(s). It will not generate dummy variables with drop_first=True.")

    if not cols_for_dummies:
        print(f"Skipping Bucket {b} as neither 'Time_Group' nor 'region' has enough unique values to create meaningful dummy variables for the model.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - insufficient unique values for dummies"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - insufficient unique values for dummies"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - insufficient unique values for dummies"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - insufficient unique values for dummies"
        continue

    dummies = pd.get_dummies(df_b[cols_for_dummies], prefix=['time', 'region'], drop_first=True)

    if dummies.empty and not cols_for_dummies: # Should be caught above, but as a safeguard
        print(f"Skipping Bucket {b} as no dummy variables were created (this state should have been caught earlier).")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - no dummy variables created (safeguard)"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - no dummy variables created (safeguard)"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - no dummy variables created (safeguard)"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - no dummy variables created (safeguard)"
        continue

    # Identify baseline levels after dummy creation for this bucket
    # For 'Time_Group'
    actual_time_groups = df_b['Time_Group'].unique()
    dummy_time_cols_generated = [col for col in dummies.columns if col.startswith('time_')]
    baseline_time_group = "Unknown"
    if len(actual_time_groups) > 1 and len(dummy_time_cols_generated) == len(actual_time_groups) -1:
        for tg in actual_time_groups:
            if f"time_{tg}" not in dummy_time_cols_generated:
                baseline_time_group = tg
                break
    elif len(actual_time_groups) == 1:
        baseline_time_group = actual_time_groups[0] + " (Only Group)"

    # For 'region'
    actual_regions = df_b['region'].unique()
    dummy_region_cols_generated = [col for col in dummies.columns if col.startswith('region_')]
    baseline_region = "Unknown"
    if len(actual_regions) > 1 and len(dummy_region_cols_generated) == len(actual_regions) -1 :
        for r_val in actual_regions:
            if f"region_{r_val}" not in dummy_region_cols_generated:
                baseline_region = r_val
                break
    elif len(actual_regions) == 1:
        baseline_region = actual_regions[0] + " (Only Region)"

    print(f"Bucket {b}: Baseline Time_Group (due to drop_first=True): {baseline_time_group}")
    print(f"Bucket {b}: Baseline Region (due to drop_first=True): {baseline_region}")
    all_results[f"Bucket_{b}_Baseline_Time_Group"] = baseline_time_group
    all_results[f"Bucket_{b}_Baseline_Region"] = baseline_region


    time_cols = [col for col in dummies.columns if col.startswith('time_')]
    region_cols = [col for col in dummies.columns if col.startswith('region_')]

    # Step 6: Generate interaction terms
    interaction_terms_dict = {}
    if time_cols and region_cols: # Only generate if both time and region dummies were created
        print("Generating interaction terms...")
        for t in tqdm(time_cols, desc=f"Interaction Terms (Bucket {b})"):
            for r in region_cols:
                interaction_terms_dict[f"{t}_x_{r}"] = dummies[t] * dummies[r]
    else:
        print("Skipping interaction term generation as either time_cols or region_cols (or both) are empty.")

    interaction_df = pd.DataFrame(interaction_terms_dict, index=dummies.index) # Ensure index alignment
    if not interaction_df.empty:
        interaction_df = interaction_df.loc[:, (interaction_df != 0).any(axis=0)]

    # Step 7: Construct model matrix
    # Concatenate only available parts: dummies might be present even if interactions are not
    features_to_concat = []
    if not dummies.empty:
        features_to_concat.append(dummies)
    if not interaction_df.empty:
        features_to_concat.append(interaction_df)

    if not features_to_concat:
        print(f"Skipping Bucket {b} as no features (dummies or interactions) are available to construct X_b_temp.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - no features for X_b_temp"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - no features for X_b_temp"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - no features for X_b_temp"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - no features for X_b_temp"
        continue

    X_b_temp = pd.concat(features_to_concat, axis=1)

    if 'view_count' not in df_b.columns: # Should be caught earlier
        print(f"Critical Error: 'view_count' missing in df_b for Bucket {b} at Step 7.")
        continue
    y_b_temp = np.log1p(df_b['view_count']).astype(float) # y_b_temp will have index from df_b

    if X_b_temp.empty: # Should be caught by features_to_concat check
        print(f"Skipping Bucket {b} as feature matrix X_b_temp is empty before adding constant.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - X_b_temp empty"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - X_b_temp empty"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - X_b_temp empty"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - X_b_temp empty"
        continue

    # Align X_b_temp with y_b_temp *before* adding constant, if their indices might differ
    # (though pd.concat should preserve index from `dummies` which comes from `df_b`)
    aligned_idx = X_b_temp.index.intersection(y_b_temp.index)
    X_b_temp_aligned = X_b_temp.loc[aligned_idx]
    y_b_temp_aligned = y_b_temp.loc[aligned_idx]

    if X_b_temp_aligned.empty or y_b_temp_aligned.empty:
        print(f"Skipping Bucket {b} as alignment resulted in empty X or y.")
        # ... store skipped status ...
        continue

    X_b_with_const = sm.add_constant(X_b_temp_aligned)

    # Step 8: Clean and ensure all numeric
    X_b_clean = X_b_with_const.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)

    # Final alignment before VIF and OLS (using X_b_clean's index as it might have dropped all-NaN rows if any)
    final_common_index = y_b_temp_aligned.index.intersection(X_b_clean.index)
    y_b_aligned = y_b_temp_aligned.loc[final_common_index]
    X_b_aligned = X_b_clean.loc[final_common_index]

    if y_b_aligned.empty:
        print(f"Skipping Bucket {b} as target variable y_b_aligned is empty after final alignment.")
        # ... (store skipped status for all relevant keys in all_results) ...
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - y_b_aligned empty post-final-align"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - y_b_aligned empty post-final-align"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - y_b_aligned empty post-final-align"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - y_b_aligned empty post-final-align"
        continue
    if X_b_aligned.shape[0] != y_b_aligned.shape[0]:
        print(f"CRITICAL WARNING for Bucket {b}: X_b_aligned ({X_b_aligned.shape[0]} rows) and y_b_aligned ({y_b_aligned.shape[0]} rows) have different number of rows after final alignment. This indicates a potential data loss or misalignment. Skipping.")
        # ... (store skipped status) ...
        continue
    if X_b_aligned.shape[1] <= 1: # Should have 'const' and at least one predictor
        print(f"Skipping Bucket {b} due to insufficient features after cleaning and final alignment (only 'const' or less). Found {X_b_aligned.shape[1]} columns.")
        # ... (store skipped status) ...
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - insufficient features post-final-align"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - insufficient features post-final-align"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - insufficient features post-final-align"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - insufficient features post-final-align"
        continue

    # Step 9: VIF calculation
    print(f"\nCalculating VIF for ALL terms in Bucket {b}...")
    X_for_vif = pd.DataFrame() # Initialize to empty
    if 'const' in X_b_aligned.columns:
        X_for_vif = X_b_aligned.drop(columns=['const'])
    else:
        print(f"Warning: 'const' column not found in X_b_aligned for Bucket {b} prior to VIF. Attempting VIF on X_b_aligned as is.")
        X_for_vif = X_b_aligned.copy()

    if X_for_vif.empty or X_for_vif.shape[1] < 2:
        vif_skip_reason = "X_for_vif empty" if X_for_vif.empty else f"<2 features (found {X_for_vif.shape[1]})"
        print(f"Skipping VIF for Bucket {b} as {vif_skip_reason}.")
        all_results[f"Bucket_{b}_VIF_All"] = f"Skipped - {vif_skip_reason} for VIF"
    else:
        vif_data_all = pd.DataFrame()
        vif_data_all["Variable"] = X_for_vif.columns
        print(f"Calculating VIF for {X_for_vif.shape[1]} variables in Bucket {b}...")
        try:
            vif_values_all = []
            for i in tqdm(range(X_for_vif.shape[1]), desc=f"VIF All (Bucket {b})"):
                vif_values_all.append(variance_inflation_factor(X_for_vif.values, i))
            vif_data_all["VIF"] = vif_values_all

            print(f"\n--- VIF Results for ALL Terms in Bucket {b} ---")
            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                print(vif_data_all.sort_values(by='VIF', ascending=False))
            all_results[f"Bucket_{b}_VIF_All"] = vif_data_all.sort_values(by='VIF', ascending=False)

            interaction_vif_data = vif_data_all[vif_data_all['Variable'].str.contains('_x_', case=False, na=False)].copy()
            if not interaction_vif_data.empty:
                print(f"\n--- VIF Results for Interaction Terms Only in Bucket {b} ---")
                with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                    print(interaction_vif_data.sort_values(by='VIF', ascending=False))
            else:
                print(f"\nNo interaction terms (containing '_x_') found to display VIF separately for Bucket {b}.")

        except Exception as e:
            print(f"❌ Error calculating VIF for ALL terms in Bucket {b}: {e}")
            print("Debug info: X_for_vif.shape =", X_for_vif.shape)
            if not X_for_vif.empty:
                 print("X_for_vif head (up to 5 rows, 10 columns for debugging):\n", X_for_vif.iloc[:5, :min(10, X_for_vif.shape[1])])
                 print("X_for_vif dtypes:\n", X_for_vif.dtypes)
                 print("X_for_vif describe:\n", X_for_vif.describe(include='all'))
            all_results[f"Bucket_{b}_VIF_All"] = f"Error: {e}"

    # Step 10: Fit OLS model
    print(f"\nFitting OLS model for Bucket {b}...")
    model_fitted_successfully = False
    try:
        if X_b_aligned.empty or y_b_aligned.empty:
             raise ValueError("Cannot fit model with empty X or y aligned data.")
        if X_b_aligned.shape[0] < X_b_aligned.shape[1]: # More features than samples
             raise ValueError(f"Cannot fit model: more features ({X_b_aligned.shape[1]}) than samples ({X_b_aligned.shape[0]}).")

        model = sm.OLS(y_b_aligned, X_b_aligned).fit() # X_b_aligned should already include 'const'
        print(f"\n--- Model Summary for Bucket {b} ---")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):
            print(model.summary())
        all_results[f"Bucket_{b}_ModelSummary"] = model.summary().as_text()
        model_fitted_successfully = True

    except Exception as e:
        print(f"❌ Model fitting failed for Bucket {b}: {e}")
        all_results[f"Bucket_{b}_ModelSummary"] = f"Failed: {e}"
        all_results[f"Bucket_{b}_Plot"] = f"Skipped - model fitting failed: {e}" # Skip plot if model fails
        all_results[f"Bucket_{b}_Recommendations"] = f"Skipped - model fitting failed: {e}" # Skip recommendations

    if model_fitted_successfully:
        # Step 11: Plot actual vs. predicted (Example)
        try:
            print(f"\n--- Generating Actual vs. Predicted Plot for Bucket {b} ---")
            y_pred = model.predict(X_b_aligned)
            plt.figure(figsize=(10, 6))
            sns.scatterplot(x=y_b_aligned, y=y_pred, alpha=0.5)
            plt.plot([y_b_aligned.min(), y_b_aligned.max()], [y_b_aligned.min(), y_b_aligned.max()], 'r--', lw=2) # Diagonal line
            plt.xlabel("Actual Log(View Count + 1)")
            plt.ylabel("Predicted Log(View Count + 1)")
            plt.title(f"Bucket {b}: Actual vs. Predicted Log View Counts")
            plt.grid(True)
            plot_filename = f"bucket_{b}_actual_vs_predicted.png"
            plt.savefig(plot_filename)
            print(f"Saved plot: {plot_filename}")
            # files.download(plot_filename) # Uncomment to download in Colab
            plt.show()
            all_results[f"Bucket_{b}_Plot"] = f"Plot generated: {plot_filename}"
        except Exception as e:
            print(f"❌ Plotting failed for Bucket {b}: {e}")
            all_results[f"Bucket_{b}_Plot"] = f"Plotting failed: {e}"

        # Step 12 & 13: Extract Effects and Recommend (Adapted from your 5_13update_mlr (1).py)
        try:
            print(f"\n--- Extracting Effects and Generating Recommendations for Bucket {b} ---")
            params_df = model.params.reset_index()
            params_df.columns = ['Feature', 'Coefficient']
            pvalues_df = model.pvalues.reset_index()
            pvalues_df.columns = ['Feature', 'P_Value'] # Renamed from 'P-value' for consistency

            effects_summary_df = pd.merge(params_df, pvalues_df, on='Feature')

            # Filter for significant interaction terms (example: p-value < 0.05)
            significant_effects = effects_summary_df[
                (effects_summary_df['P_Value'] < 0.05) &
                (effects_summary_df['Feature'].str.contains('_x_')) & # Focus on interactions
                (effects_summary_df['Feature'] != 'const')
            ].copy() # Use .copy() to avoid SettingWithCopyWarning

            recommendations_for_bucket = []
            if not significant_effects.empty:
                print(f"Significant interaction effects for Bucket {b} (P < 0.05):")
                with pd.option_context('display.max_rows', None): print(significant_effects)

                # Logic to parse 'time_X_x_region_Y' into TimeGroup and Region
                # Assumes format like "time_GROUP_x_region_REGIONNAME"
                def parse_interaction_term(term):
                    parts = term.split('_x_')
                    time_part = parts[0].replace('time_', '')
                    region_part = parts[1].replace('region_', '')
                    return time_part, region_part

                significant_effects[['TimeGroup_Interaction', 'Region_Interaction']] = significant_effects['Feature'].apply(
                    lambda x: pd.Series(parse_interaction_term(x))
                )

                # Store structured recommendations
                for idx, row in significant_effects.iterrows():
                    recommendations_for_bucket.append({
                        "Region": row['Region_Interaction'],
                        "Time_Group_Interaction_with": row['TimeGroup_Interaction'], # This is the non-baseline time group
                        "Baseline_Time_Group_for_Context": baseline_time_group, # From Step 5
                        "Baseline_Region_for_Context": baseline_region, # From Step 5
                        "Interaction_Coefficient": row['Coefficient'],
                        "P_Value": row['P_Value'],
                        "Interpretation_Note": f"Interaction effect of TimeGroup {row['TimeGroup_Interaction']} (vs {baseline_time_group}) AND Region {row['Region_Interaction']} (vs {baseline_region})."
                    })
                all_results[f"Bucket_{b}_Recommendations_Structured"] = recommendations_for_bucket

                # Simplified best time group recommendation based on largest positive significant interaction
                # This is a very basic interpretation; a more nuanced one might be needed.
                best_recommendation_overall = {}
                if not significant_effects[significant_effects['Coefficient'] > 0].empty:
                    best_positive_interaction = significant_effects[significant_effects['Coefficient'] > 0].sort_values(by='Coefficient', ascending=False).iloc[0]
                    best_recommendation_overall = {
                        "Best_Observed_Interaction_Region": best_positive_interaction['Region_Interaction'],
                        "Best_Observed_Interaction_TimeGroup": best_positive_interaction['TimeGroup_Interaction'],
                        "Interaction_Coefficient": best_positive_interaction['Coefficient'],
                        "P_Value": best_positive_interaction['P_Value'],
                        "Note": "This suggests the strongest positive significant interaction."
                    }
                else:
                    best_recommendation_overall = {"Message": "No significant positive interaction effects found for recommendation."}
                all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = best_recommendation_overall
                print("\nSimplified Best Positive Interaction Recommendation:")
                print(best_recommendation_overall)

            else:
                print(f"No significant interaction effects (p < 0.05) found for Bucket {b}.")
                all_results[f"Bucket_{b}_Recommendations_Structured"] = "No significant interaction effects (p < 0.05)"
                all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = "No significant interaction effects (p < 0.05)"

        except Exception as e:
            print(f"❌ Effects extraction or recommendation generation failed for Bucket {b}: {e}")
            all_results[f"Bucket_{b}_Recommendations_Structured"] = f"Failed: {e}"
            all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = f"Failed: {e}"


# After the loop, you can inspect all_results more thoroughly
print("\n\n--- Summary of All Processed Buckets (Final Review) ---")
for key, value in all_results.items():
    print(f"\n--- {key} ---")
    if isinstance(value, pd.DataFrame):
        if "VIF" in key:
            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                print(value)
        else:
            with pd.option_context('display.max_rows', 30): # Show more rows for other DFs
                print(value)
    elif isinstance(value, list): # For structured recommendations
        print("List of recommendations/data:")
        for item in value:
            print(item)
    elif isinstance(value, dict): # For single dict recommendations
        print("Recommendation/data dictionary:")
        for k_dict, v_dict in value.items():
            print(f"  {k_dict}: {v_dict}")
    elif isinstance(value, str): # For model summaries (text) or error/skipped messages
        print(value)
    else:
        print(f"Result is of type {type(value)}, not specifically formatted for display.")

print("\n--- Processing Complete ---")

# -*- coding: utf-8 -*-
"""MLR_try2fix_5.17_updated_full_with_diag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/YOUR_DRIVE_LINK_HERE
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
# from sklearn.preprocessing import LabelEncoder # Not actively used for VIF/OLS logic
import matplotlib.pyplot as plt # For plotting
import seaborn as sns # For plotting
from tqdm import tqdm # For progress bars
from google.colab import files # Specific to Google Colab

# Initialize the dictionary to store results from each bucket
all_results = {}

# Step 1: Upload the file
# If running locally, replace this with:
# df_original_data = pd.read_excel("your_file_path.xlsx")
print("Please upload your Excel file.")
uploaded = files.upload()
excel_file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {excel_file_name}")
df_original_data = pd.read_excel(excel_file_name)
df = df_original_data.copy() # Work on a copy

# Step 2: Basic preprocessing
df = df.rename(columns={
    'categoryid': 'categoryId',
    'Time_group': 'Time_Group' # Ensure consistency if this is the actual column name
})

# Filter out unwanted data
# Ensure 'view_count' and 'categoryId' columns exist
if 'categoryId' not in df.columns or 'view_count' not in df.columns:
    raise ValueError("Required columns 'categoryId' or 'view_count' not found in the DataFrame.")

df = df[(df['categoryId'].astype(str) != '24') & (df['view_count'] > 0)]

# Convert columns to appropriate types for dummy encoding and processing
if 'categoryId' in df.columns:
    df['categoryId'] = df['categoryId'].astype(str)

required_cols_for_dummies = ['Time_Group', 'region', 'bucket_label']
for col_name in required_cols_for_dummies:
    if col_name not in df.columns:
        raise ValueError(f"Required column '{col_name}' not found. Please check the column names in your Excel file.")

df['Time_Group'] = df['Time_Group'].astype(str)
df['region'] = df['region'].astype(str)
try:
    df['bucket_label'] = df['bucket_label'].astype(int)
except ValueError as e:
    raise ValueError(f"Error converting 'bucket_label' to int. Please check its values. Original error: {e}")


# Step 3 (Implicit): Define baseline levels if needed for interpretation later
# This depends on how pd.get_dummies with drop_first=True behaves.

# Step 4: Loop by bucket_label
unique_buckets_in_data = sorted(df['bucket_label'].unique())
buckets_to_process = unique_buckets_in_data # Process all unique buckets found
print(f"Found unique buckets: {unique_buckets_in_data}. Processing: {buckets_to_process}")


for b in buckets_to_process:
    print(f"\n--- Processing Bucket {b} ---")
    # df_b is used for modelling, df_b_original_for_diagnosis is for checking original counts
    df_b = df[df['bucket_label'] == b].copy()
    df_b_original_for_diagnosis = df[df['bucket_label'] == b].copy() # For INF VIF diagnosis

    print(f"Samples in Bucket {b}: {df_b.shape[0]}")

    if df_b.empty:
        print(f"Skipping Bucket {b} due to no data.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - no data"
        all_results[f"Bucket_{b}_ModelSummary"] = "Skipped - no data"
        all_results[f"Bucket_{b}_Plot"] = "Skipped - no data"
        all_results[f"Bucket_{b}_Recommendations"] = "Skipped - no data"
        all_results[f"Bucket_{b}_INF_VIF_Diagnosis"] = "Skipped - no data"
        continue

    # Get actual unique values for Time_Group and region within the current bucket
    unique_time_groups_in_bucket = df_b['Time_Group'].nunique()
    unique_regions_in_bucket = df_b['region'].nunique()

    # Step 5: One-hot encode Time_Group and region
    cols_for_dummies_creation = []
    if unique_time_groups_in_bucket > 1:
        cols_for_dummies_creation.append('Time_Group')
    else:
        print(f"Bucket {b}: 'Time_Group' has only {unique_time_groups_in_bucket} unique value(s). It will not generate dummy variables with drop_first=True.")

    if unique_regions_in_bucket > 1:
        cols_for_dummies_creation.append('region')
    else:
        print(f"Bucket {b}: 'region' has only {unique_regions_in_bucket} unique value(s). It will not generate dummy variables with drop_first=True.")

    if not cols_for_dummies_creation:
        print(f"Skipping Bucket {b} as neither 'Time_Group' nor 'region' has enough unique values to create meaningful dummy variables for the model.")
        all_results[f"Bucket_{b}_VIF_All"] = "Skipped - insufficient unique values for dummies"
        # ... (store skipped for other keys)
        continue

    dummies = pd.get_dummies(df_b[cols_for_dummies_creation], prefix=['time', 'region'], drop_first=True)

    # Identify baseline levels after dummy creation for this bucket
    baseline_time_group = "N/A or Single Group"
    if 'Time_Group' in cols_for_dummies_creation:
        actual_time_groups = df_b['Time_Group'].unique()
        dummy_time_cols_generated = [col for col in dummies.columns if col.startswith('time_')]
        if len(actual_time_groups) > 1: # Only if we attempted to create dummies for it
            found_baseline = False
            for tg_val_original in sorted(list(actual_time_groups)): # Sort for consistency
                if f"time_{tg_val_original}" not in dummy_time_cols_generated:
                    baseline_time_group = str(tg_val_original)
                    found_baseline = True
                    break
            if not found_baseline and dummy_time_cols_generated: # Should not happen with drop_first=True if >1 unique
                 baseline_time_group = "Error: Baseline not identified but dummies exist"
        elif len(actual_time_groups) == 1:
            baseline_time_group = str(actual_time_groups[0]) + " (Only Group)"
    else: # Time_Group was not dummified
        if unique_time_groups_in_bucket == 1:
            baseline_time_group = str(df_b['Time_Group'].unique()[0]) + " (Only Group, not dummified)"

    baseline_region = "N/A or Single Region"
    if 'region' in cols_for_dummies_creation:
        actual_regions = df_b['region'].unique()
        dummy_region_cols_generated = [col for col in dummies.columns if col.startswith('region_')]
        if len(actual_regions) > 1: # Only if we attempted to create dummies for it
            found_baseline = False
            for r_val_original in sorted(list(actual_regions)): # Sort for consistency
                if f"region_{r_val_original}" not in dummy_region_cols_generated:
                    baseline_region = str(r_val_original)
                    found_baseline = True
                    break
            if not found_baseline and dummy_region_cols_generated:
                baseline_region = "Error: Baseline not identified but dummies exist"
        elif len(actual_regions) == 1:
            baseline_region = str(actual_regions[0]) + " (Only Region)"
    else: # region was not dummified
        if unique_regions_in_bucket == 1:
             baseline_region = str(df_b['region'].unique()[0]) + " (Only Region, not dummified)"


    print(f"Bucket {b}: Baseline Time_Group (due to drop_first=True or single group): {baseline_time_group}")
    print(f"Bucket {b}: Baseline Region (due to drop_first=True or single group): {baseline_region}")
    all_results[f"Bucket_{b}_Baseline_Time_Group"] = baseline_time_group
    all_results[f"Bucket_{b}_Baseline_Region"] = baseline_region

    time_cols = [col for col in dummies.columns if col.startswith('time_')]
    region_cols = [col for col in dummies.columns if col.startswith('region_')]

    # Step 6: Generate interaction terms
    interaction_terms_dict = {}
    if time_cols and region_cols: # Only generate if both time and region dummies were created
        print("Generating interaction terms...")
        for t_col_name in tqdm(time_cols, desc=f"Interaction Terms (Bucket {b})"):
            for r_col_name in region_cols:
                interaction_terms_dict[f"{t_col_name}_x_{r_col_name}"] = dummies[t_col_name] * dummies[r_col_name]
    else:
        print("Skipping interaction term generation as either time_cols or region_cols (or both) are effectively empty for interaction.")

    interaction_df = pd.DataFrame(interaction_terms_dict, index=dummies.index) # Ensure index alignment
    if not interaction_df.empty:
        interaction_df = interaction_df.loc[:, (interaction_df != 0).any(axis=0)] # Remove all-zero interaction columns

    # Step 7: Construct model matrix
    features_to_concat = []
    if not dummies.empty:
        features_to_concat.append(dummies)
    if not interaction_df.empty:
        features_to_concat.append(interaction_df)

    if not features_to_concat:
        print(f"Skipping Bucket {b} as no features (dummies or interactions) are available to construct X_b_temp.")
        # ... (store skipped for all relevant keys in all_results) ...
        continue

    X_b_temp = pd.concat(features_to_concat, axis=1)

    y_b_temp = np.log1p(df_b['view_count']).astype(float)

    aligned_idx = X_b_temp.index.intersection(y_b_temp.index)
    X_b_temp_aligned = X_b_temp.loc[aligned_idx]
    y_b_temp_aligned = y_b_temp.loc[aligned_idx]

    if X_b_temp_aligned.empty or y_b_temp_aligned.empty:
        print(f"Skipping Bucket {b} as alignment resulted in empty X or y.")
        # ... (store skipped status) ...
        continue

    X_b_with_const = sm.add_constant(X_b_temp_aligned, has_constant='add') # Ensure constant is added

    # Step 8: Clean and ensure all numeric
    X_b_clean = X_b_with_const.apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)

    final_common_index = y_b_temp_aligned.index.intersection(X_b_clean.index)
    y_b_aligned = y_b_temp_aligned.loc[final_common_index]
    X_b_aligned = X_b_clean.loc[final_common_index]

    if y_b_aligned.empty or X_b_aligned.empty or X_b_aligned.shape[0] != y_b_aligned.shape[0] or X_b_aligned.shape[1] <= 1 :
        print(f"Skipping Bucket {b} due to data issues after final alignment (empty, mismatch, or insufficient features). y_shape: {y_b_aligned.shape}, X_shape: {X_b_aligned.shape}")
        # ... (store skipped status) ...
        continue

    # Step 9: VIF calculation
    vif_df_for_bucket_vif_calc = None # Initialize
    print(f"\nCalculating VIF for ALL terms in Bucket {b}...")
    X_for_vif_calc = pd.DataFrame()
    if 'const' in X_b_aligned.columns:
        X_for_vif_calc = X_b_aligned.drop(columns=['const'])
    else:
        print(f"Warning: 'const' column not found in X_b_aligned for Bucket {b} prior to VIF. Attempting VIF on X_b_aligned as is.")
        X_for_vif_calc = X_b_aligned.copy()

    if X_for_vif_calc.empty or X_for_vif_calc.shape[1] < 1: # VIF can be calculated for 1 var, though usually for >1
        vif_skip_reason = "X_for_vif_calc empty" if X_for_vif_calc.empty else f"<1 feature (found {X_for_vif_calc.shape[1]})"
        print(f"Skipping VIF for Bucket {b} as {vif_skip_reason}.")
        all_results[f"Bucket_{b}_VIF_All"] = f"Skipped - {vif_skip_reason} for VIF"
    else:
        vif_data_all_calc = pd.DataFrame()
        vif_data_all_calc["Variable"] = X_for_vif_calc.columns
        print(f"Calculating VIF for {X_for_vif_calc.shape[1]} variables in Bucket {b}...")
        try:
            vif_values_all_calc = []
            # For VIF, at least 2 variables are typically needed for meaningful interpretation.
            # If only one variable, VIF is usually 1.0 by definition with variance_inflation_factor if it's a single column df.
            # However, the function variance_inflation_factor expects exog to be a 2d array with at least 2 columns.
            # If X_for_vif_calc has only one column, we'll report VIF as 1.0.
            if X_for_vif_calc.shape[1] == 1:
                print("Only one variable for VIF calculation. VIF is 1.0 by definition.")
                vif_values_all_calc = [1.0] * X_for_vif_calc.shape[1]
            else:
                for i in tqdm(range(X_for_vif_calc.shape[1]), desc=f"VIF All (Bucket {b})"):
                    vif_values_all_calc.append(variance_inflation_factor(X_for_vif_calc.values, i))

            vif_data_all_calc["VIF"] = vif_values_all_calc
            vif_df_for_bucket_vif_calc = vif_data_all_calc.sort_values(by='VIF', ascending=False)

            print(f"\n--- VIF Results for ALL Terms in Bucket {b} ---")
            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                print(vif_df_for_bucket_vif_calc)
            all_results[f"Bucket_{b}_VIF_All"] = vif_df_for_bucket_vif_calc

            interaction_vif_data = vif_df_for_bucket_vif_calc[vif_df_for_bucket_vif_calc['Variable'].str.contains('_x_', case=False, na=False)].copy()
            if not interaction_vif_data.empty:
                print(f"\n--- VIF Results for Interaction Terms Only in Bucket {b} ---")
                with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                    print(interaction_vif_data.sort_values(by='VIF', ascending=False))
            else:
                print(f"\nNo interaction terms (containing '_x_') found to display VIF separately for Bucket {b}.")
        except Exception as e:
            print(f"❌ Error calculating VIF for ALL terms in Bucket {b}: {e}")
            # (Debugging info as before)
            all_results[f"Bucket_{b}_VIF_All"] = f"Error: {e}"
            vif_df_for_bucket_vif_calc = pd.DataFrame({'Variable':[], 'VIF':[]}) # Ensure it's a DataFrame for diagnosis

    # === DIAGNOSTIC CODE FOR INF VIFS START ===
    if vif_df_for_bucket_vif_calc is not None and not vif_df_for_bucket_vif_calc.empty:
        print(f"\n--- Diagnosing INF VIFs for Bucket {b} ---")
        inf_vif_terms = vif_df_for_bucket_vif_calc[
            (vif_df_for_bucket_vif_calc['VIF'] == np.inf) &
            (vif_df_for_bucket_vif_calc['Variable'].str.contains('_x_', case=False, na=False))
        ]['Variable'].tolist()

        if not inf_vif_terms:
            print(f"No interaction terms with INF VIF found in Bucket {b} to diagnose.")
            all_results[f"Bucket_{b}_INF_VIF_Diagnosis"] = "No INF VIF interaction terms to diagnose."
        else:
            print(f"Found {len(inf_vif_terms)} interaction terms with INF VIF in Bucket {b}. Checking sample counts...")
            missing_or_sparse_combinations_diag = {}
            for term_diag in inf_vif_terms:
                try:
                    # Assumes term format: "time_TIMEV_x_region_REGIONV"
                    parts_diag = term_diag.split('_x_') # e.g. ["time_TIMEV", "region_REGIONV"]
                    # Further split to get actual values
                    # time_actual_value = parts_diag[0].split('time_')[-1]
                    # region_actual_value = parts_diag[1].split('region_')[-1]

                    # Robust parsing in case original Time_Group/region values contain '_'
                    # We need to reconstruct the original values that were used to *create* the dummy variable names
                    # This is tricky if original values had '_'. The current dummy names are time_ORIGINALTIME and region_ORIGINALREGION
                    # And interaction is time_ORIGINALTIME_x_region_ORIGINALREGION

                    # Strategy: Identify which parts of the interaction term name belong to 'time' and 'region' based on the dummy prefixes
                    # This assumes `time_cols` and `region_cols` contain the dummy variable names (e.g., 'time_A', 'region_X')

                    parsed_time_val_diag = None
                    parsed_region_val_diag = None

                    # Example: term_diag = "time_Morning_Shift_x_region_North_America"
                    # time_cols might contain "time_Morning_Shift"
                    # region_cols might contain "region_North_America"

                    possible_time_component = parts_diag[0] # e.g., "time_Morning_Shift"
                    possible_region_component = parts_diag[1] # e.g., "region_North_America"

                    # We need the original values, not the dummy var names with prefixes for lookup in df_b_original_for_diagnosis
                    # This requires knowing how the dummy names were formed from original values.
                    # Assuming `time_cols` and `region_cols` (from step 5) hold the dummy variable names.

                    # Let's re-evaluate parsing based on the original column names from dummy creation
                    # The interaction term `term_diag` is like "dummy_time_col_name_x_dummy_region_col_name"

                    # Simplistic split for now, assuming prefixes "time_" and "region_" were consistently used
                    # and original values don't clash too badly with this.

                    # If term_diag = "time_1_x_region_JP"
                    # time_part_from_term = "time_1"
                    # region_part_from_term = "region_JP"

                    # The dummy variable names are what `term_diag` is composed of.
                    # We need to strip the prefixes to get the original values.

                    # Reconstruct the original values that would have led to these dummy names
                    # The interaction term is formed by dummy_time_name + "_x_" + dummy_region_name
                    # So, parts_diag[0] is the dummy_time_name, parts_diag[1] is the dummy_region_name

                    time_dummy_name_diag = parts_diag[0] # e.g. time_10
                    region_dummy_name_diag = parts_diag[1] # e.g. region_CA

                    # Now, strip the prefixes to get the original values for lookup
                    # This assumes simple prefixing. If original values had underscores, this needs more care.
                    time_val_for_lookup = time_dummy_name_diag.replace('time_', '', 1)
                    region_val_for_lookup = region_dummy_name_diag.replace('region_', '', 1)

                    count_diag = df_b_original_for_diagnosis[
                        (df_b_original_for_diagnosis['Time_Group'] == time_val_for_lookup) &
                        (df_b_original_for_diagnosis['region'] == region_val_for_lookup)
                    ].shape[0]

                    print(f"  Interaction term: {term_diag}")
                    print(f"    Parsed for lookup -> Time_Group: '{time_val_for_lookup}', Region: '{region_val_for_lookup}'")
                    print(f"    Sample count in Bucket {b} (original data): {count_diag}")

                    if count_diag == 0:
                        missing_or_sparse_combinations_diag[term_diag] = f"ZERO samples (Time_Group='{time_val_for_lookup}', Region='{region_val_for_lookup}')"
                    elif count_diag < 5: # Threshold for sparse
                        missing_or_sparse_combinations_diag[term_diag] = f"VERY LOW samples ({count_diag}) (Time_Group='{time_val_for_lookup}', Region='{region_val_for_lookup}')"
                except Exception as e_diag:
                    print(f"  Error parsing or counting for VIF INF term '{term_diag}': {e_diag}")
                    missing_or_sparse_combinations_diag[term_diag] = f"Error during diagnosis: {e_diag}"

            if missing_or_sparse_combinations_diag:
                print("\nSummary of INF VIF interaction terms in Bucket {b} likely due to missing/sparse data (based on original counts):")
                for term_sum, reason_sum in missing_or_sparse_combinations_diag.items():
                    print(f"  - {term_sum}: {reason_sum}")
                all_results[f"Bucket_{b}_INF_VIF_Diagnosis"] = missing_or_sparse_combinations_diag
            else:
                print("\nNo obvious missing/sparse data issues found for INF VIF interaction terms based on direct counts in original bucket data.")
                all_results[f"Bucket_{b}_INF_VIF_Diagnosis"] = "INF VIF interaction terms present, but not obviously due to zero/low counts in original data for those combinations. Check for other complex dependencies or issues if dummies for single categories were unintentionally created and then interacted."
    else:
        print(f"Skipping INF VIF diagnosis for Bucket {b} as VIF DataFrame is not available or empty.")
        all_results[f"Bucket_{b}_INF_VIF_Diagnosis"] = "Skipped - VIF data not available/empty."
    # === DIAGNOSTIC CODE FOR INF VIFS END ===

    # Step 10: Fit OLS model
    print(f"\nFitting OLS model for Bucket {b}...")
    model_fitted_successfully = False
    try:
        if X_b_aligned.empty or y_b_aligned.empty:
             raise ValueError("Cannot fit model with empty X or y aligned data.")
        if X_b_aligned.shape[0] < X_b_aligned.shape[1]:
             raise ValueError(f"Cannot fit model: more features ({X_b_aligned.shape[1]}) than samples ({X_b_aligned.shape[0]}).")

        model = sm.OLS(y_b_aligned, X_b_aligned).fit()
        print(f"\n--- Model Summary for Bucket {b} ---")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):
            print(model.summary())
        all_results[f"Bucket_{b}_ModelSummary"] = model.summary().as_text()
        model_fitted_successfully = True

    except Exception as e:
        print(f"❌ Model fitting failed for Bucket {b}: {e}")
        all_results[f"Bucket_{b}_ModelSummary"] = f"Failed: {e}"
        all_results[f"Bucket_{b}_Plot"] = f"Skipped - model fitting failed: {e}"
        all_results[f"Bucket_{b}_Recommendations_Structured"] = f"Skipped - model fitting failed: {e}"
        all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = f"Skipped - model fitting failed: {e}"


    if model_fitted_successfully:
        # Step 11: Plot actual vs. predicted
        try:
            print(f"\n--- Generating Actual vs. Predicted Plot for Bucket {b} ---")
            y_pred = model.predict(X_b_aligned)
            plt.figure(figsize=(10, 6))
            sns.scatterplot(x=y_b_aligned, y=y_pred, alpha=0.5)
            plt.plot([y_b_aligned.min(), y_b_aligned.max()], [y_b_aligned.min(), y_b_aligned.max()], 'r--', lw=2)
            plt.xlabel("Actual Log(View Count + 1)")
            plt.ylabel("Predicted Log(View Count + 1)")
            plt.title(f"Bucket {b}: Actual vs. Predicted Log View Counts (R-squared: {model.rsquared:.3f})")
            plt.grid(True)
            plot_filename = f"bucket_{b}_actual_vs_predicted.png"
            plt.savefig(plot_filename)
            print(f"Saved plot: {plot_filename}")
            # files.download(plot_filename) # Uncomment to download in Colab
            plt.show()
            all_results[f"Bucket_{b}_Plot"] = f"Plot generated: {plot_filename}"
        except Exception as e:
            print(f"❌ Plotting failed for Bucket {b}: {e}")
            all_results[f"Bucket_{b}_Plot"] = f"Plotting failed: {e}"

        # Step 12 & 13: Extract Effects and Recommend
        try:
            print(f"\n--- Extracting Effects and Generating Recommendations for Bucket {b} ---")
            params_df = model.params.reset_index()
            params_df.columns = ['Feature', 'Coefficient']
            pvalues_df = model.pvalues.reset_index()
            pvalues_df.columns = ['Feature', 'P_Value']

            effects_summary_df = pd.merge(params_df, pvalues_df, on='Feature')

            significant_effects = effects_summary_df[
                (effects_summary_df['P_Value'] < 0.05) &
                (effects_summary_df['Feature'].str.contains('_x_')) &
                (effects_summary_df['Feature'] != 'const')
            ].copy()

            recommendations_for_bucket_structured = []
            if not significant_effects.empty:
                print(f"Significant interaction effects for Bucket {b} (P < 0.05):")
                with pd.option_context('display.max_rows', None): print(significant_effects)

                def parse_interaction_term_for_recommend(term, known_time_dummies, known_region_dummies):
                    # term is like "time_VAL1_x_region_VAL2"
                    parts = term.split('_x_')
                    time_dummy_name = parts[0] # e.g. "time_Morning"
                    region_dummy_name = parts[1] # e.g. "region_US"

                    # Strip prefix to get original-like value
                    time_group_interacted = time_dummy_name.replace('time_', '', 1)
                    region_interacted = region_dummy_name.replace('region_', '', 1)
                    return time_group_interacted, region_interacted

                significant_effects[['TimeGroup_Interaction', 'Region_Interaction']] = significant_effects['Feature'].apply(
                    lambda x: pd.Series(parse_interaction_term_for_recommend(x, time_cols, region_cols)) # Pass dummy lists for context if needed by parser
                )

                for idx, row in significant_effects.iterrows():
                    recommendations_for_bucket_structured.append({
                        "Feature_Term": row['Feature'],
                        "Interacted_Region": row['Region_Interaction'],
                        "Interacted_Time_Group": row['TimeGroup_Interaction'],
                        "Baseline_Region_Context": baseline_region,
                        "Baseline_Time_Group_Context": baseline_time_group,
                        "Interaction_Coefficient": row['Coefficient'],
                        "P_Value": row['P_Value'],
                        "Interpretation_Note": (f"Effect of TimeGroup '{row['TimeGroup_Interaction']}' (vs baseline '{baseline_time_group}') "
                                                f"AND Region '{row['Region_Interaction']}' (vs baseline '{baseline_region}') "
                                                f"on log(view_count+1) is {row['Coefficient']:.4f}.")
                    })
                all_results[f"Bucket_{b}_Recommendations_Structured"] = recommendations_for_bucket_structured

                best_recommendation_overall = {}
                positive_significant_effects = significant_effects[significant_effects['Coefficient'] > 0]
                if not positive_significant_effects.empty:
                    best_positive_interaction = positive_significant_effects.sort_values(by='Coefficient', ascending=False).iloc[0]
                    best_recommendation_overall = {
                        "Best_Observed_Interaction_Feature": best_positive_interaction['Feature'],
                        "Best_Interacted_Region": best_positive_interaction['Region_Interaction'],
                        "Best_Interacted_TimeGroup": best_positive_interaction['TimeGroup_Interaction'],
                        "Interaction_Coefficient": best_positive_interaction['Coefficient'],
                        "P_Value": best_positive_interaction['P_Value'],
                        "Note": "This combination showed the strongest positive significant interaction effect relative to double baselines."
                    }
                else:
                    best_recommendation_overall = {"Message": "No significant positive interaction effects found for recommendation."}
                all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = best_recommendation_overall
                print("\nSimplified Best Positive Interaction Recommendation (relative to baselines):")
                print(best_recommendation_overall)

            else:
                print(f"No significant interaction effects (p < 0.05) found for Bucket {b}.")
                all_results[f"Bucket_{b}_Recommendations_Structured"] = "No significant interaction effects (p < 0.05)"
                all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = "No significant interaction effects (p < 0.05)"

        except Exception as e:
            print(f"❌ Effects extraction or recommendation generation failed for Bucket {b}: {e}")
            all_results[f"Bucket_{b}_Recommendations_Structured"] = f"Failed: {e}"
            all_results[f"Bucket_{b}_Recommendations_Best_Positive_Interaction"] = f"Failed: {e}"


# After the loop, print all collected results
print("\n\n--- Detailed Summary of All Processed Buckets (Final Review) ---")
for key, value in all_results.items():
    print(f"\n--- {key} ---")
    if isinstance(value, pd.DataFrame):
        if "VIF_All" in key or "INF_VIF_Diagnosis" in key: # For VIF tables, show all rows
            with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
                print(value)
        else: # For other DataFrames, like effects, a snippet might be okay or adjust as needed
            with pd.option_context('display.max_rows', 50, 'display.max_columns', None, 'display.width', 1000):
                print(value)
    elif isinstance(value, list): # For structured recommendations (list of dicts)
        print("List of items:")
        if not value:
            print("  (empty list)")
        for item_idx, item_val in enumerate(value):
            print(f"  Item {item_idx + 1}:")
            if isinstance(item_val, dict):
                for k_dict, v_dict in item_val.items():
                    print(f"    {k_dict}: {v_dict}")
            else:
                print(f"    {item_val}")
    elif isinstance(value, dict): # For single dict recommendations or diagnosis summary
        print("Dictionary content:")
        if not value:
            print("  (empty dictionary)")
        for k_dict, v_dict in value.items():
            print(f"  {k_dict}: {v_dict}")
    elif isinstance(value, str): # For model summaries (text) or error/skipped messages
        print(value)
    else:
        print(f"Result is of type {type(value)}, raw value: {value}")

print("\n--- Processing Complete ---")