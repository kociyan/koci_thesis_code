# -*- coding: utf-8 -*-
"""Thesis_data_preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9BiLhibxkVKNRuQtWS_DPAzSWfHg3hs
"""

#region列添加国籍/地区信息使用

# 安装依赖（Colab中如果已安装会自动跳过）
!pip install pandas openpyxl

import pandas as pd
import os
from google.colab import files

def extract_country_code(filename):
    """智能提取国家代码的强化版"""
    # 移除文件扩展名
    base_name = os.path.splitext(filename)[0]

    # 常见国家代码模式识别
    patterns = [
        r'\b([A-Z]{2})\b',    # 匹配全大写的2字母代码
        r'\b([A-Z]{3})\b',    # 匹配全大写的3字母代码
        r'^([A-Za-z]{2})_',   # 开头两个字母+下划线
        r'_([A-Za-z]{2})\b'   # 下划线+结尾两个字母
    ]

    # 优先检测已知平台前缀
    platform_prefixes = ['youtube', 'yt', 'video', 'trending']
    for prefix in platform_prefixes:
        if prefix in base_name.lower():
            base_name = base_name.lower().split(prefix)[0]

    # 尝试用分隔符拆分
    for sep in ['_', '-', ' ', '#']:
        parts = base_name.split(sep)
        for part in parts:
            clean_part = part.strip().upper()
            if 2 <= len(clean_part) <= 3 and clean_part.isalpha():
                return clean_part

    # 最终回退方案
    return "MODIFIED"

# 上传文件
uploaded = files.upload()
original_filename = next(iter(uploaded))
print(f"📤 已上传文件：{original_filename}")

# 智能编码检测（带异常处理）
try:
    # Check file extension and use appropriate reader
    if original_filename.lower().endswith('.xlsx'):
        df = pd.read_excel(original_filename)
        print("✅ 使用 pd.read_excel 成功读取文件")
    else:
        df = pd.read_csv(original_filename, encoding='utf-8-sig')
        print("✅ 使用 utf-8-sig 编码成功读取文件")
except UnicodeDecodeError:
    try:
        df = pd.read_csv(original_filename, encoding='ISO-8859-1')
        print("✅ 使用 ISO-8859-1 编码成功读取文件")
    except Exception as e:
        print(f"❌ 无法读取文件，错误类型：{type(e).__name__}")
        raise

# 列定位双保险机制
target_col = None

# 方案一：按J列位置定位（第10列）
if df.shape[1] > 9:
    position_col = df.columns[9]
    if 'region' in position_col.lower():
        target_col = position_col
        print(f"🔍 通过列位置定位到目标列：{position_col}")

# 方案二：智能名称匹配
if not target_col:
    name_variants = ['region', 'reg', 'countrycode', '地区']
    for col in df.columns:
        clean_col = col.strip().lower()
        if any(variant in clean_col for variant in name_variants):
            target_col = col
            print(f"🔍 通过名称匹配到目标列：{col}")
            break

# 执行替换操作
if target_col:
    df[target_col] = 'JP'  # 此处用于调整国籍或地区缩写

    # 生成标准化文件名
    country_code = extract_country_code(original_filename)
    output_filename = f"{country_code}_youtube_trending.csv"

    # 确保CSV格式
    if not output_filename.lower().endswith('.csv'):
        output_filename += '.csv'

    # 保存文件
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    files.download(output_filename)

    print(f"""
    🎉 操作成功！
    → 目标列：{target_col}
    → 生成文件：{output_filename}
    → 总处理行数：{len(df):,}
    """)
else:
    print("""
    ❗ 未找到目标列，可能原因：
    1. 数据列位置发生变化
    2. 列名不包含'region'相关关键词
    3. 文件格式异常

    🔍 当前列结构：
    """)
    print(df.dtypes.to_string())
    print("\n💡 请截图当前数据表结构，我将提供定制解决方案")

#这个用来打时间差标记
import pandas as pd
import chardet
from google.colab import files

# 检测文件编码
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        raw_data = f.read(100000)
    bom_encodings = {b'\xff\xfe': 'utf-16-le', b'\xfe\xff': 'utf-16-be', b'\xef\xbb\xbf': 'utf-8-sig'}
    for bom, encoding in bom_encodings.items():
        if raw_data.startswith(bom):
            return encoding
    result = chardet.detect(raw_data)
    return result['encoding'] if result['confidence'] > 0.8 else None

# 读取CSV文件，确保正确处理特殊字符和缺失值
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'gb18030', 'cp1252']
    if encoding and encoding not in encodings_to_try:
        encodings_to_try.insert(0, encoding)
    for codec in encodings_to_try:
        try:
            return pd.read_csv(file_path, encoding=codec, engine='python', dtype=str, keep_default_na=False, na_values=[''])
        except Exception:
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

df['publishedAt'] = pd.to_datetime(df['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')
df['trending_date'] = pd.to_datetime(df['trending_date'], format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')

df['time_diff_hours'] = (df['trending_date'] - df['publishedAt']).dt.total_seconds() / 3600

def categorize_time_diff(hours):
    if pd.isna(hours):
        return '不可用'
    if 0 <= hours <= 12:
        return '12h'
    elif 12 < hours <= 24:
        return '12-24h'
    elif 24 < hours <= 48:
        return '24-48h'
    elif hours > 48:
        return '48+h'
    else:
        return '不可用'

df['时间差分类'] = df['time_diff_hours'].apply(categorize_time_diff)

# 还原 publishedAt 和 trending_date 的格式
df['publishedAt'] = df['publishedAt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = df['trending_date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# 确保 video_id 以字符串方式处理，并正确识别所有内容
df['video_id'] = df['video_id'].astype(str).fillna('UNKNOWN')
df['组号'] = df.groupby(df['video_id'], sort=False, dropna=False, group_keys=False).ngroup() + 1

# 定义排序优先级
time_diff_order = {'12h': 1, '12-24h': 2, '24-48h': 3, '48+h': 4, '不可用': 5}
df['时间差排序'] = df['时间差分类'].map(time_diff_order)

# 先按组号排序，再按时间差分类排序
df.sort_values(by=['组号', '时间差排序'], inplace=True)

# 删除临时排序列
df.drop(columns=['时间差排序'], inplace=True)

output_filename = f"processed_{input_filename}"
df.to_csv(output_filename, index=False, encoding='utf-8-sig')

print("\n✅ 处理完成，开始下载文件...")
files.download(output_filename)
print(df[['video_id', '组号', 'publishedAt', 'trending_date', 'time_diff_hours', '时间差分类']].head())

#处理按照video_id分组的问题，但是这份代码不能准确识别video_id，有很多NAME?错误存在。
import pandas as pd
import chardet
import csv
from google.colab import files

# 增强版编码检测（处理BOM头）
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# 安全读取CSV（增强对特殊字符的处理）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # 强制指定为字符串类型
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 清理video_id列（处理Excel公式注入问题）
df['video_id'] = (
    df['video_id']
    .astype(str)
    .str.strip()
    .str.replace(r'^=', "'=", regex=True)  # 在等号前添加单引号防止Excel解析
    .str.replace(r'\x00', '', regex=True)    # 移除空字符
)

# 日期处理（带错误检查）
def parse_datetime(col):
    try:
        return pd.to_datetime(col, format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')
    except Exception as e:
        print(f"日期解析错误：{str(e)}")
        return pd.NaT

df['publishedAt_dt'] = parse_datetime(df['publishedAt'])
df['trending_date_dt'] = parse_datetime(df['trending_date'])

# 时间差计算（小时）
df['time_diff_hours'] = (
    (df['trending_date_dt'] - df['publishedAt_dt'])
    .dt.total_seconds()
    .div(3600)
    .round(2)
)

# 时间差分类逻辑
def categorize_time_diff(hours):
    if pd.isna(hours):
        return '不可用'
    if 0 <= hours <= 12:
        return '12h'
    elif 12 < hours <= 24:
        return '12-24h'
    elif 24 < hours <= 48:
        return '24-48h'
    elif hours > 48:
        return '48+h'
    else:
        return '不可用'

df['时间差分类'] = df['time_diff_hours'].apply(categorize_time_diff)

# 分组逻辑（确保同一video_id保持相同组号）
df['组号'] = (
    df.groupby('video_id', sort=False, dropna=False)
    .ngroup()
    .add(1)
    .astype(str)
    .str.zfill(4)  # 生成4位数字组号，如0001
)

# 排序逻辑（带优先级）
time_diff_order = {'12h': 1, '12-24h': 2, '24-48h': 3, '48+h': 4, '不可用': 5}
df['_时间差排序'] = df['时间差分类'].map(time_diff_order)

# 最终排序（组号 > 时间差分类 > 发布时间）
df = df.sort_values(
    by=['组号', '_时间差排序', 'publishedAt_dt'],
    ascending=[True, True, True]
).drop(columns=['_时间差排序', 'publishedAt_dt', 'trending_date_dt'])

# 恢复原始日期格式
df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = pd.to_datetime(df['trending_date']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# 导出设置（防止Excel公式注入）
output_filename = f"processed_{input_filename}"
df.to_csv(
    output_filename,
    index=False,
    encoding='utf-8-sig',
    quoting=csv.QUOTE_ALL,  # 所有字段加引号
    escapechar='\\',        # 处理包含引号的情况
    date_format='%Y-%m-%dT%H:%M:%SZ'
)

print("\n✅ 处理完成，开始下载文件...")
files.download(output_filename)

# 显示示例数据（验证处理结果）
print("\n处理结果示例：")
display(df[['video_id', '组号', 'publishedAt', 'trending_date', 'time_diff_hours', '时间差分类']].head(3))

import pandas as pd
import chardet
import csv
import re
from google.colab import files

# 增强版编码检测（处理BOM头）
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# 安全读取CSV（强化类型指定）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': 'string'},  # 使用pandas的string类型
                keep_default_na=False,
                na_filter=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"⛔ 解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 核心ID清洗函数
def sanitize_video_id(series):
    return (
        series
        .astype(str)  # Convert the column to string explicitly
        .str.strip()
        # 处理公式注入风险
        .replace(r'^[=+\-@]', lambda x: f"'{x.group(0) if x.group(0) is not None else ''}", regex=True)  # Handle potential empty matches
        # 移除控制字符
        .str.replace(r'[\x00-\x1F\x7F]', '', regex=True)
        # 处理超长ID
        .apply(lambda x: f"'{x}" if len(str(x)) > 15 and x.isdigit() else x)
    )

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 执行ID清洗（带日志记录）
original_ids = df['video_id'].copy()
df['video_id'] = sanitize_video_id(df['video_id'])

# 对比检查清洗结果
changed_mask = original_ids != df['video_id']
if changed_mask.any():
    print("\n🔍 检测到以下video_id被修改：")
    display(pd.concat([
        original_ids[changed_mask].rename('原始ID'),
        df['video_id'][changed_mask].rename('处理后ID')
    ], axis=1).head(5))

# 日期处理（增强容错）
def safe_datetime_parse(s, format):
    try:
        return pd.to_datetime(s, format=format, utc=True)
    except Exception as e:
        print(f"⚠️ 日期解析错误：{e}\n样例值：{s.head(1).values}")
        return pd.to_datetime(s, errors='coerce', utc=True)

df['publishedAt_dt'] = safe_datetime_parse(df['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')
df['trending_date_dt'] = safe_datetime_parse(df['trending_date'], '%Y-%m-%dT%H:%M:%SZ')

# 时间差计算（精确到分钟）
df['time_diff'] = (df['trending_date_dt'] - df['publishedAt_dt']).dt.total_seconds() / 60
df['time_diff_hours'] = (df['time_diff'] / 60).round(2)

# 时间差分类（动态区间）
bins = [0, 12, 24, 48, float('inf')]
labels = ['12h', '12-24h', '24-48h', '48+h']
df['时间差分类'] = pd.cut(
    df['time_diff_hours'],
    bins=bins,
    labels=labels,
    right=False,
    include_lowest=True
).cat.add_categories('不可用').fillna('不可用')

# 分组逻辑（稳定哈希分组）
df['组号'] = (
    df.groupby('video_id', sort=False, dropna=False)
    .ngroup()
    .add(1)
    .astype(str)
    .str.zfill(4)
)

# 排序逻辑（多级排序）
df = df.sort_values(
    by=['组号', 'time_diff_hours', 'publishedAt_dt'],
    ascending=[True, True, True]
)

# 还原原始日期格式
df['publishedAt'] = df['publishedAt_dt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = df['trending_date_dt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# 清理中间列
df = df.drop(columns=['publishedAt_dt', 'trending_date_dt', 'time_diff'])

# 导出配置（Excel兼容格式）
output_filename = f"processed_{input_filename}"
df.to_csv(
    output_filename,
    index=False,
    encoding='utf-8-sig',
    quoting=csv.QUOTE_ALL,
    escapechar='\\',
    date_format='%Y-%m-%dT%H:%M:%SZ',
    # newline='\r\n'  # Remove or comment out this line
)

print("\n✅ 处理完成，开始下载文件...")
files.download(output_filename)

# 最终验证
print("\n🔎 最终数据验证：")
print(f"总记录数：{len(df)}")
print("video_id 样例：")
print(df['video_id'].sample(5).to_string(index=False))

from google.colab import files
import pandas as pd
from tqdm import tqdm
import io

# 交互式上传文件
uploaded = files.upload()
file_name = next(iter(uploaded))
df = pd.read_csv(io.BytesIO(uploaded[file_name]))  # 关键修改点

# 构建 channelId 到有效 video_id 的映射
print("\n步骤 1/2: 分析 channelId 对应的有效 video_id...")
channel_id_map = {}
valid_channels = df[df['channelId'] != '#NAME?']
unique_channel_ids = valid_channels['channelId'].unique()

for channel_id in tqdm(unique_channel_ids, desc="分析进度"):
    video_ids = valid_channels[valid_channels['channelId'] == channel_id]['video_id']
    video_ids_clean = video_ids[video_ids != '#NAME?']

    if video_ids_clean.nunique() == 1:
        channel_id_map[channel_id] = video_ids_clean.iloc[0]
    else:
        channel_id_map[channel_id] = None

# 替换无效的 video_id
print("\n步骤 2/2: 修复无效的 video_id...")
need_fix_mask = (df['video_id'] == '#NAME?') & (df['channelId'].isin(channel_id_map))
to_fix_indices = df[need_fix_mask].index.tolist()

for idx in tqdm(to_fix_indices, desc="修复进度"):
    channel_id = df.loc[idx, 'channelId']
    if channel_id_map.get(channel_id):
        df.loc[idx, 'video_id'] = channel_id_map[channel_id]

# 保存并下载结果（保持CSV格式）
output_filename = f"processed_{file_name}"
df.to_csv(output_filename, index=False)
files.download(output_filename)

print("\n处理完成！更新后的文件已自动下载。")

#通过channelid和publishat反查video_id,解决部分的#NAME?问题
import pandas as pd
import numpy as np
import tqdm
from google.colab import files
from io import BytesIO
from tqdm.notebook import tqdm

# 交互式上传文件
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# 判断文件类型并读取
if filename.endswith('.csv'):
    df = pd.read_csv(BytesIO(uploaded[filename]))  # 读取 CSV
else:
    df = pd.read_excel(BytesIO(uploaded[filename]), engine='openpyxl')  # 读取 Excel（兼容 .xlsx）

# 进度条初始化
tqdm.pandas()

def replace_name(row, df):
    if row['video_id'] == '#NAME?':
        channel_id = row['channelId']
        if channel_id != '#NAME?':
            # 查找相同 channelId 的所有行
            matching_rows = df[df['channelId'] == channel_id]
            unique_video_ids = matching_rows['video_id'].unique()

            # 移除 '#NAME?'
            unique_video_ids = [vid for vid in unique_video_ids if vid != '#NAME?']

            if len(unique_video_ids) == 1:
                return unique_video_ids[0]  # 只有一个唯一的 video_id
            else:
                # 查找 publishedAt 是否有重复的
                duplicate_published = matching_rows[matching_rows.duplicated('publishedAt', keep=False)]
                if not duplicate_published.empty:
                    return duplicate_published.iloc[0]['video_id']  # 取第一个匹配的 video_id
    return row['video_id']

# 处理数据并显示进度
df['video_id'] = df.progress_apply(lambda row: replace_name(row, df), axis=1)

# 保存处理后的文件
output_filename = "processed_file.xlsx"
df.to_excel(output_filename, index=False, engine='openpyxl')

# 自动下载
files.download(output_filename)
print("处理完成，文件已自动下载！")

#删掉所有的'不可用'
import pandas as pd
import numpy as np
from google.colab import files

# 增强版编码检测（处理BOM头）
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# 安全读取CSV（增强对特殊字符的处理）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # 强制指定为字符串类型
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. 删除“时间差分类”列中值为“不可用”的行
df = df[df['时间差分类'] != '不可用']

# 2. 按照 video_id 分组，并按照 time_diff_hours 进行排序
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['组号'] = np.nan  # 初始化组号列

group_id = 1  # 初始组号
group_mapping = {}  # 存储 video_id 对应的组号

# 遍历数据，按 video_id 赋予组号
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, '组号'] = group_mapping[vid]

# 3. 处理 video_id 为 “#NAME?” 的情况
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # 计算正常的组数

df.loc[name_mask, '组号'] = np.arange(num_existing_groups + 1, num_existing_groups + 1 + name_mask.sum())

# 4. 重新排序，确保 #NAME? 的数据在最后
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

#按照(trend-publish)time排列的video_id重新分组,并将所有的NAME?单独划组
import pandas as pd
import numpy as np
from google.colab import files

# 安全读取CSV（增强对特殊字符的处理）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # 强制指定为字符串类型
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. 删除“时间差分类”列中值为“不可用”的行
#initial_row_count = len(df)
#df = df[df['时间差分类'] != '不可用']
#deleted_rows = initial_row_count - len(df)
#print(f"删除的行数: {deleted_rows}")

# 2. 按照 video_id 分组，并按照 time_diff_hours 进行排序
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['组号'] = np.nan  # 初始化组号列

group_id = 1  # 初始组号
group_mapping = {}  # 存储 video_id 对应的组号

# 遍历数据，按 video_id 赋予组号
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, '组号'] = group_mapping[vid]

# 3. 处理 video_id 为 “#NAME?” 的情况
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # 计算正常的组数

# 为每个 "#NAME?" 单独分配一个组号
name_indices = df.index[name_mask]
for i, idx in enumerate(name_indices):
    df.at[idx, '组号'] = num_existing_groups + 1 + i

# 4. 重新排序，确保 #NAME? 的数据在最后
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 输出分组数
total_groups = df['组号'].nunique()
print(f"总分组数: {total_groups}")

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

#按照(trend-publish)time排列的video_id重新分组,并将所有的NAME?单独划组
import pandas as pd
import numpy as np
from google.colab import files
import chardet # Added import statement
import csv
# 安全读取CSV（增强对特殊字符的处理）
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# 安全读取CSV（增强对特殊字符的处理）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # 强制指定为字符串类型
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. 删除“时间差分类”列中值为“不可用”的行
#initial_row_count = len(df)
#df = df[df['时间差分类'] != '不可用']
#deleted_rows = initial_row_count - len(df)
#print(f"删除的行数: {deleted_rows}")

# 2. 按照 video_id 分组，并按照 time_diff_hours 进行排序
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['组号'] = np.nan  # 初始化组号列

group_id = 1  # 初始组号
group_mapping = {}  # 存储 video_id 对应的组号

# 遍历数据，按 video_id 赋予组号
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, '组号'] = group_mapping[vid]

# 3. 处理 video_id 为 “#NAME?” 的情况
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # 计算正常的组数

# 为每个 "#NAME?" 单独分配一个组号
name_indices = df.index[name_mask]
for i, idx in enumerate(name_indices):
    df.at[idx, '组号'] = num_existing_groups + 1 + i

# 4. 重新排序，确保 #NAME? 的数据在最后
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 输出分组数
total_groups = df['组号'].nunique()
print(f"总分组数: {total_groups}")

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

#上面的代码还是不行，再度修复
import pandas as pd
import numpy as np
from google.colab import files

# 交互式上传文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 1. 删除“时间差分类”列中值为“不可用”的行
initial_row_count = len(df)
df = df[df['时间差分类'] != '不可用']
deleted_rows = initial_row_count - len(df)
print(f"删除的行数: {deleted_rows}")

# 2. 处理 video_id 为 “#NAME?” 的情况
name_mask = df['video_id'] == '#NAME?'
name_indices = df.index[name_mask]

# 直接修改 video_id，确保每一行唯一
df.loc[name_indices, 'video_id'] = [f"#NAME? {i+1}" for i in range(len(name_indices))]

# 3. 按照 video_id 分组，并按照 time_diff_hours 进行排序
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['组号'] = np.nan  # 初始化组号列

group_id = 1  # 初始组号
group_mapping = {}  # 存储 video_id 对应的组号

# 遍历数据，按 video_id 赋予组号
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, '组号'] = group_mapping[vid]

# 4. 重新排序，确保 #NAME? 的数据在最后
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 输出分组数
total_groups = df['组号'].nunique()
print(f"总分组数: {total_groups}")

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

import pandas as pd
from google.colab import files

# 交互式上传文件
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# 读取数据并预处理
df = pd.read_excel(filename)
df = df[df['时间差分类'] != '不可用']

# 分离正常数据和异常数据
normal_df = df[df['video_id'] != '#NAME?'].copy()
name_df = df[df['video_id'] == '#NAME?'].copy()

# 处理正常数据的分组
if not normal_df.empty:
    # 按video_id分组并排序
    normal_df = normal_df.sort_values(['video_id', 'time_diff_hours'])
    # 分配组号（从1开始）
    normal_df['组号'] = normal_df.groupby('video_id').ngroup() + 1
else:
    normal_df['组号'] = pd.NA

# 处理异常数据的分组
if not name_df.empty:
    # 生成唯一video_id（rename1, rename2...）
    name_df = name_df.sort_values('time_diff_hours')
    name_df['video_id'] = [f'rename{i+1}' for i in range(len(name_df))]

    # 获取最大已有组号
    max_group = normal_df['组号'].max() if not normal_df.empty else 0

    # 为每个异常行分配独立组号
    name_df['组号'] = range(max_group + 1, max_group + 1 + len(name_df))

# 合并数据并最终排序
final_df = pd.concat([normal_df, name_df]).sort_values(
    ['组号', 'time_diff_hours'],
    ignore_index=True
)

# 验证输出
print(f"总分组数：{final_df['组号'].nunique()}")
print("\n前5个正常组样例：")
print(final_df[final_df['组号'] <= max_group].head())
print("\n异常组样例：")
print(final_df[final_df['组号'] > max_group].head())

# 导出文件
output_filename = 'processed_data_v3.xlsx'
final_df.to_excel(output_filename, index=False)
files.download(output_filename)

#每次都要打开文件手动解决NAME?的分组问题，这个始终无法通过代码实现。

#查找rename行中B与C同时重复的值，并将它们重新分组
import pandas as pd
import numpy as np
from google.colab import files

# 交互式上传文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 1. 筛选出 video_id 含有 "rename" 的行
rename_mask = df['video_id'].astype(str).str.contains("rename", case=False, na=False)
rename_df = df[rename_mask].copy()

# 2. 识别 publishedAt 和 channelId 同时重复的行
grouped = rename_df.groupby(['publishedAt', 'channelId'])
fix_id = 1  # fix 组计数
fix_mapping = {}  # 存储相同 publishedAt-channelId 组合的组号

for _, group in grouped:
    if len(group) > 1:  # 仅处理重复项
        group_indices = group.index
        fix_label = f"fix{fix_id}"
        rename_df.loc[group_indices, 'video_id'] = fix_label  # 修改 video_id
        fix_mapping[fix_label] = fix_id  # 记录组号
        fix_id += 1

# 3. 重新分配组号
rename_df['组号'] = np.nan  # 重新初始化组号列
rename_groups = rename_df['video_id'].unique()
group_id = 1

group_dict = {}  # 存储 group_id 对应的组号
for vid in rename_groups:
    group_dict[vid] = group_id
    group_id += 1

rename_df['组号'] = rename_df['video_id'].map(group_dict)

# 4. 确保 fix 组号排在 rename 组号之后
df.update(rename_df)  # 更新原 DataFrame
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

print("处理完成，文件已生成并可下载。")

#重新处理分组问题
import pandas as pd
import numpy as np
from google.colab import files

# 交互式上传文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 1. 筛选出 video_id 含有 "rename" 的行
rename_mask = df['video_id'].astype(str).str.contains("rename", case=False, na=False)
rename_df = df[rename_mask].copy()

# 2. 识别 publishedAt 和 channelId 同时重复的行
grouped = rename_df.groupby(['publishedAt', 'channelId'])
fix_id = 1  # fix 组计数
fix_mapping = {}  # 存储相同 publishedAt-channelId 组合的组号

for _, group in grouped:
    if len(group) > 1:  # 仅处理重复项
        group_indices = group.index
        fix_label = f"fix{fix_id}"
        rename_df.loc[group_indices, 'video_id'] = fix_label  # 修改 video_id
        fix_mapping[fix_label] = fix_id  # 记录组号
        fix_id += 1

# 3. 重新分配组号
rename_df['组号'] = np.nan  # 重新初始化组号列
rename_groups = rename_df['video_id'].unique()
existing_max_group_id = df['组号'].max() if pd.notna(df['组号']).any() else 0  # 找到当前最大组号

group_id = existing_max_group_id + 1  # fix 组号应排在所有现有组号之后
fix_group_dict = {}  # 存储 fix 组的组号

for vid in rename_groups:
    fix_group_dict[vid] = group_id
    group_id += 1

rename_df['组号'] = rename_df['video_id'].map(fix_group_dict)

df.update(rename_df)  # 更新原 DataFrame

# 4. 重新执行 video_id 归组，确保所有相同 video_id 归为同一组
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)
df['组号'] = np.nan  # 重新初始化组号

group_id = 1  # 重新计算组号
group_mapping = {}  # 存储 video_id 对应的组号

for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, '组号'] = group_mapping[vid]

# 5. 确保 fix 组号排在所有其他组号之后
df.sort_values(by=['组号', 'time_diff_hours'], inplace=True)

# 导出处理后的 Excel 文件
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

print("处理完成，文件已生成并可下载。")

#localtime转换有效

# 安装依赖包（如果尚未安装）
!pip install pytz pandas

# --- 完整代码开始 ---
import pandas as pd
import pytz
from google.colab import files

# 安全读取CSV（增强对特殊字符的处理）
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # 强制指定为字符串类型
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"解析错误（编码 {codec}）: {str(e)}")
            continue
    raise ValueError("所有编码尝试失败")

# 上传文件
print("🔼 请上传CSV文件：")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 3. 解析UTC时间列为时区敏感对象
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. 定义时区转换函数
def get_timezone(region_code):
    try:
        return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"警告: 无法找到国家代码 '{region_code}' 的时区")
        return None

# 5. 转换时间并分割为日期和时间列
def convert_and_split_time(row):
    tz = get_timezone(row["region"])
    if not tz:
        return pd.NaT, pd.NaT

    # 转换为本地时间并移除时区信息
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # 分割为日期和时间字符串（格式：YYYY-MM-DD 和 HH:MM:SS）
    return local_time.date().isoformat(), local_time.time().strftime("%H:%M:%S")

# 应用转换函数生成两列
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 6. 清理原始UTC时间列的时区信息（可选）
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 7. 显示结果预览
print("\n转换结果预览:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 8. 保存并下载（完全兼容Excel）
output_file = "output_split_datetime.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- 完整代码结束 ---

#多时区国家转换成localtime取人口最多的时区
# 安装依赖包（如果尚未安装）
!pip install pytz pandas

# --- 完整代码开始 ---
import pandas as pd
import pytz
from google.colab import files

# 1. 上传Excel文件（Colab交互式弹窗）
uploaded = files.upload()
file_name = next(iter(uploaded))  # 获取上传文件名

# 2. 读取Excel数据
df = pd.read_excel(file_name, engine='openpyxl')

# 3. 解析UTC时间列为时区敏感对象
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. 定义多时区国家的主时区映射表（人口最多/最常用）
MAIN_TIMEZONES = {
    # 国家代码 : 指定时区
    "US": "America/New_York",     # 美国东部（覆盖44%人口）
    "CA": "America/Toronto",      # 加拿大东部（安大略省）
    "BR": "America/Sao_Paulo",    # 巴西东南部（圣保罗）
    "IN": "Asia/Kolkata",         # 印度标准时间（IST）
    "RU": "Europe/Moscow",        # 俄罗斯西部（莫斯科）
    "MX": "America/Mexico_City"   # 墨西哥中部
}

def get_timezone(region_code):
    try:
        # 优先使用预定义的主时区
        if region_code in MAIN_TIMEZONES:
            return pytz.timezone(MAIN_TIMEZONES[region_code])
        # 其他国家选择第一个时区
        return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"警告: 无法找到国家代码 '{region_code}' 的时区")
        return None

# 5. 时间转换与分割（精确到秒）
def convert_and_split_time(row):
    tz = get_timezone(row["region"])
    if not tz:
        return None, None

    # 转换为本地时间（保留秒）
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # 分割为日期和时间字符串
    return (
        local_time.date().isoformat(),         # localdate列：YYYY-MM-DD
        local_time.time().strftime("%H:%M:%S") # localtime列：HH:MM:SS
    )

# 生成两列
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 6. 清理原始UTC时间列的时区信息（确保Excel兼容）
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 7. 显示结果预览
print("\n转换结果预览:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 8. 保存并下载
output_file = "output_multicountry_time.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- 完整代码结束 ---

#墨西哥2022.10.30取消夏令时，时间检查
# 安装依赖包（如果尚未安装）
!pip install pytz pandas

# --- 完整代码开始 ---
import pandas as pd
import pytz
from datetime import datetime
from google.colab import files

# 1. 上传Excel文件（Colab交互式弹窗）
uploaded = files.upload()
file_name = next(iter(uploaded))  # 获取上传文件名

# 2. 读取Excel数据
df = pd.read_excel(file_name)

# 3. 解析UTC时间列为时区敏感对象
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. 定义多时区国家的主时区映射表
MAIN_TIMEZONES = {
    "US": "America/New_York",
    "CA": "America/Toronto",
    "BR": "America/Sao_Paulo",
    "IN": "Asia/Kolkata",
    "RU": "Europe/Moscow",
    "MX": "America/Mexico_City"
}

# 5. 墨西哥时区特殊处理函数（修复点1：正确初始化分界时间）
def get_mexico_timezone(utc_time):
    # 墨西哥政策变更分界点（UTC时间）
    cutoff_date = pd.Timestamp("2022-10-30T00:00:00Z").tz_convert("UTC")  # 修复拼写和时区方法

    if utc_time <= cutoff_date:
        return pytz.timezone("America/Mexico_City")
    else:
        # 创建固定UTC-6的时区（无夏令时）
        return pytz.FixedOffset(-6*60)  # UTC-6

# 6. 动态获取时区的函数（修复点2：传递正确的时区参数）
def get_timezone(region_code, utc_time):
    try:
        if region_code == "MX":
            return get_mexico_timezone(utc_time)
        elif region_code in MAIN_TIMEZONES:
            return pytz.timezone(MAIN_TIMEZONES[region_code])
        else:
            return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"警告: 无法找到国家代码 '{region_code}' 的时区")
        return None

# 7. 时间转换与分割（精确到秒）
def convert_and_split_time(row):
    tz = get_timezone(row["region"], row["publishedAt"])  # 修复点3：确保传入时区敏感对象
    if not tz:
        return None, None

    # 转换为本地时间并移除时区信息
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # 分割为日期和时间字符串
    return (
        local_time.date().isoformat(),
        local_time.time().strftime("%H:%M:%S")
    )

# 应用转换函数生成两列
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 8. 清理原始UTC时间列的时区信息
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 9. 显示结果预览
print("\n转换结果预览:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 10. 保存并下载
output_file = "output_timezone_fixed.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- 完整代码结束 ---

#检查两份MX的内容是否有差异
# 安装依赖包（如果尚未安装）
!pip install pandas

# --- 完整代码开始 ---
import pandas as pd
from google.colab import files

def compare_excel_files():
    # 1. 上传第一个文件
    print("请上传第一个Excel文件：")
    uploaded_file1 = files.upload()
    file1_name = next(iter(uploaded_file1))

    # 2. 上传第二个文件
    print("\n请上传第二个Excel文件：")
    uploaded_file2 = files.upload()
    file2_name = next(iter(uploaded_file2))

    # 3. 读取两个文件
    try:
        df1 = pd.read_excel(file1_name)
        df2 = pd.read_excel(file2_name)
    except Exception as e:
        print(f"\n错误: 文件读取失败 - {str(e)}")
        return

    # 4. 执行深度对比
    print("\n对比结果：")
    if df1.shape != df2.shape:
        print("❌ 文件结构不同 (行列数不一致)")
        print(f"文件1: {df1.shape[0]}行{df1.shape[1]}列")
        print(f"文件2: {df2.shape[0]}行{df2.shape[1]}列")
        return

    # 检查列名是否一致
    if list(df1.columns) != list(df2.columns):
        print("❌ 列名/列顺序不一致")
        print("文件1列名:", list(df1.columns))
        print("文件2列名:", list(df2.columns))
        return

    # 检查数据内容
    comparison = df1.compare(df2)
    if comparison.empty:
        print("✅ 文件内容完全一致")
    else:
        print(f"❌ 发现 {len(comparison)} 处差异")
        print("\n差异详情：")
        print(comparison)

# 执行对比
compare_excel_files()
# --- 完整代码结束 ---
#还是用回之前转换的localtime，不要用这份新代码来转化了。应该是在10.29和10.30有一个调整界限，手动整理反而会出错

#列名修改和重新排序
import pandas as pd
from google.colab import files

# 交互式上传文件
uploaded = files.upload()

# 获取上传的文件名
file_name = next(iter(uploaded))

# 读取Excel文件
df = pd.read_excel(file_name)

# 列重命名
df.rename(columns={
    df.columns[10]: 'Time_to_trend(Hours)',  # K列
    df.columns[13]: 'Local_Publishdate',     # N列
    df.columns[11]: 'Trending_time_bucket',  # L列
    df.columns[14]: 'Local_publishtime',     # O列
    df.columns[12]: 'Same_video_group'       # M列
}, inplace=True)

# 定义新列顺序（保持原始逻辑）
new_order = [9, 12, 13, 14, 5, 6, 7, 8, 3, 11, 10, 1, 4, 0, 2]
df = df.iloc[:, new_order]

# 保存并下载新文件
output_filename = 'processed_file.xlsx'
df.to_excel(output_filename, index=False)
files.download(output_filename)

print("✅ 文件处理完成！已自动下载新文件")

#去除engagement中单列0最多的
import pandas as pd
from google.colab import files

# 上传Excel文件
uploaded = files.upload()

# 读取Excel数据
file_name = next(iter(uploaded))
df = pd.read_excel(file_name)

# 计算每行单个0的数量
df['zero_count'] = (df[['view_count', 'likes', 'dislikes', 'comment_count']] == 0).sum(axis=1)

# 定义分组处理函数
def process_group(group):
    if group['zero_count'].nunique() > 1:  # 如果存在差异
        min_count = group['zero_count'].min()
        return group[group['zero_count'] == min_count]
    return group

# 按B列分组处理
result_df = df.groupby('Same_video_group', group_keys=False).apply(process_group).drop(columns=['zero_count'])

# 保存结果
result_df.to_excel('processed_result.xlsx', index=False)
files.download('processed_result.xlsx')

#将publishat，trendingdate，videoid和channelid全部清除
!pip install openpyxl xlrd==1.2.0

import pandas as pd
from google.colab import files
import os
import shutil

# 交互式上传文件
uploaded = files.upload()

# 处理每个文件
for filename in uploaded.keys():
    try:
        # 确定文件类型和读取引擎
        if filename.lower().endswith('.xlsx'):
            engine = 'openpyxl'
        elif filename.lower().endswith('.xls'):
            engine = 'xlrd'
        else:
            print(f"跳过非Excel文件: {filename}")
            continue

        # 读取Excel文件
        df = pd.read_excel(filename, engine=engine)

        # 删除L/M/N/O列（第12-15列，对应索引11-14）
        if len(df.columns) >= 15:
            cols_to_drop = df.columns[11:15]
        else:
            cols_to_drop = df.columns[11:len(df.columns)]

        df.drop(columns=cols_to_drop, inplace=True)

        # 创建新文件名并保存
        base_name = os.path.splitext(filename)[0]
        new_filename = f'useful_{base_name}.xlsx'
        df.to_excel(new_filename, index=False, engine='openpyxl')
        print(f"文件 {filename} 处理完成，保存为 {new_filename}")

    except Exception as e:
        print(f"处理 {filename} 时出错: {str(e)}")

# 打包处理后的文件
os.makedirs('processed_files', exist_ok=True)
for file in os.listdir():
    if file.startswith('useful_') and file.endswith('.xlsx'):
        shutil.move(file, os.path.join('processed_files', file))

shutil.make_archive('processed_files', 'zip', 'processed_files')

# 下载压缩包
print("\n正在准备下载链接...")
files.download('processed_files.zip')

#若trending_time_bucket中有相同的，则只保留行数最小的（时间更近的）行
import pandas as pd
import numpy as np
from google.colab import files

# 让用户上传 Excel 文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 确保所需列存在
required_columns = ['Same_video_group', 'Trending_time_bucket']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel 文件缺少必要的列: {required_columns}")

# 添加一个索引列来保留原始顺序
df['Row_Index'] = np.arange(len(df))

# 按 'Same_video_group' 和 'Trending_time_bucket' 进行分组，保留每组的第一行
filtered_df = df.sort_values(by=['Same_video_group', 'Trending_time_bucket', 'Row_Index'])
filtered_df = filtered_df.groupby(['Same_video_group', 'Trending_time_bucket'], as_index=False).first()

# 删除辅助列
filtered_df = filtered_df.drop(columns=['Row_Index'])

# 保存处理后的文件
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

print("处理完成！请下载文件。")

import pandas as pd
import numpy as np
from google.colab import files

# 让用户上传 Excel 文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 确保所需列存在
required_columns = ['Same_video_group', 'Trending_time_bucket']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel 文件缺少必要的列: {required_columns}")

# 添加一个索引列来保留原始顺序
df['Row_Index'] = np.arange(len(df))

# 按 'Same_video_group' 和 'Trending_time_bucket' 进行分组，保留每组的第一行
filtered_indices = df.groupby(['Same_video_group', 'Trending_time_bucket'])['Row_Index'].idxmin()
filtered_df = df.loc[filtered_indices].sort_values(by='Row_Index').drop(columns=['Row_Index'])

# 保存处理后的文件
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

print("处理完成！请下载文件。")

#两小时划分时间组，包前不包后
import pandas as pd
import numpy as np
from google.colab import files

# 让用户上传 Excel 文件
uploaded = files.upload()

# 获取上传的文件名
filename = list(uploaded.keys())[0]

# 读取 Excel 文件
df = pd.read_excel(filename)

# 确保所需列存在
required_columns = ['Same_video_group', 'Trending_time_bucket', 'Local_publishtime']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel 文件缺少必要的列: {required_columns}")

# 添加一个索引列来保留原始顺序
df['Row_Index'] = np.arange(len(df))

# 解析 'Local_publishtime' 并计算所属时间组
def get_time_group(time_str):
    hour = int(time_str.split(':')[0])  # 提取小时部分
    return (hour // 2) + 1  # 计算所属时间段

df['Time_Group'] = df['Local_publishtime'].astype(str).apply(get_time_group)

# 按 'Same_video_group' 和 'Trending_time_bucket' 进行分组，保留每组的第一行
filtered_indices = df.groupby(['Same_video_group', 'Trending_time_bucket'])['Row_Index'].idxmin()
filtered_df = df.loc[filtered_indices].sort_values(by='Row_Index').drop(columns=['Row_Index'])

# 保存处理后的文件
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# 提供下载链接
files.download(output_filename)

print("处理完成！请下载文件。")

#看trendingtimebucket和timegroup的分布情况
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 文件上传
uploaded = files.upload()
file_name = next(iter(uploaded.keys()))
df = pd.read_excel(file_name)

# 定义顺序常量
TREND_ORDER = ['12h', '12-24h', '24-48h', '48+h']
TIME_GROUP_ORDER = list(range(1, 13))

# 原始数据统计
print("【原始数据统计】")
print("\nJ列（Trending_time_bucket）分布：")
trend_counts = df['Trending_time_bucket'].value_counts().reindex(TREND_ORDER, fill_value=0)
print(trend_counts)

print("\nL列（Time_Group）分布：")
time_group_counts = df['Time_Group'].value_counts().reindex(TIME_GROUP_ORDER, fill_value=0)
print(time_group_counts)

# 原始数据可视化
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='Trending_time_bucket', order=TREND_ORDER)
plt.title('Trending_time_bucket 分布（原始数据）')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='Time_Group', order=TIME_GROUP_ORDER)
plt.title('Time_Group 分布（原始数据）')
plt.xticks(ticks=range(12), labels=TIME_GROUP_ORDER)
plt.show()

# 按Same_video_group去重后统计
unique_df = df.drop_duplicates(subset=['Same_video_group'])

print("\n\n【按Same_video_group去重后统计】")
print("\nJ列（Trending_time_bucket）分布：")
unique_trend_counts = unique_df['Trending_time_bucket'].value_counts().reindex(TREND_ORDER, fill_value=0)
print(unique_trend_counts)

print("\nL列（Time_Group）分布：")
unique_time_group_counts = unique_df['Time_Group'].value_counts().reindex(TIME_GROUP_ORDER, fill_value=0)
print(unique_time_group_counts)

#整理发现12h太少，48+h太多了
#如果同时有48+h和12h的，就将48+h的删掉
#如果同时存在‘24-48h’或者'12-24h'和‘48+h’，那么也将‘48+h’的整行删掉
from google.colab import files
import pandas as pd

def process_data(df):
    # 定义需要检测的冲突组合
    CONFLICT_CONDITIONS = [
        {'12h', '48+h'},  # 原始冲突条件
        {'24-48h', '48+h'},  # 新增冲突条件
    {'12-24h', '48+h'}  # 假设新增的第三种冲突条件
    ]

    final_dfs = []

    # 按视频组分组处理
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # 检查是否满足任一冲突条件
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # 处理冲突
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '48+h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# 文件上传和处理的交互流程
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# 执行处理
processed_df = process_data(original_df)

# 生成结果文件
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# 下载文件
files.download(output_filename)

print("处理完成！")
print(f"原始数据量: {len(original_df)} 行")
print(f"处理后数据量: {len(processed_df)} 行")
print(f"删除记录数: {len(original_df) - len(processed_df)} 行")
print("已下载处理后的文件：" + output_filename)

#若12h和24-48同时存在，则删24-48
from google.colab import files
import pandas as pd

def process_data(df):
    # 定义需要检测的冲突组合
    CONFLICT_CONDITIONS = [ {'12h', '24-48h'}]

    final_dfs = []

    # 按视频组分组处理
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # 检查是否满足任一冲突条件
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # 处理冲突
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '24-48h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# 文件上传和处理的交互流程
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# 执行处理
processed_df = process_data(original_df)

# 生成结果文件
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# 下载文件
files.download(output_filename)

print("处理完成！")
print(f"原始数据量: {len(original_df)} 行")
print(f"处理后数据量: {len(processed_df)} 行")
print(f"删除记录数: {len(original_df) - len(processed_df)} 行")
print("已下载处理后的文件：" + output_filename)

#若12-24与24-48同在，删24-48，RU和IN不参与
from google.colab import files
import pandas as pd

def process_data(df):
    # 定义需要检测的冲突组合
    CONFLICT_CONDITIONS = [ {'12-24h', '24-48h'}]

    final_dfs = []

    # 按视频组分组处理
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # 检查是否满足任一冲突条件
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # 处理冲突
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '24-48h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# 文件上传和处理的交互流程
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# 执行处理
processed_df = process_data(original_df)

# 生成结果文件
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# 下载文件
files.download(output_filename)

print("处理完成！")
print(f"原始数据量: {len(original_df)} 行")
print(f"处理后数据量: {len(processed_df)} 行")
print(f"删除记录数: {len(original_df) - len(processed_df)} 行")
print("已下载处理后的文件：" + output_filename)

#MX和KR的12-24分给12
from google.colab import files
import pandas as pd

def process_data(df):
    # 定义需要检测的冲突组合
    CONFLICT_CONDITIONS = [ {'12-24h', '12h'}]

    final_dfs = []

    # 按视频组分组处理
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # 检查是否满足任一冲突条件
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # 处理冲突
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '12-24h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# 文件上传和处理的交互流程
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# 执行处理
processed_df = process_data(original_df)

# 生成结果文件
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# 下载文件
files.download(output_filename)

print("处理完成！")
print(f"原始数据量: {len(original_df)} 行")
print(f"处理后数据量: {len(processed_df)} 行")
print(f"删除记录数: {len(original_df) - len(processed_df)} 行")
print("已下载处理后的文件：" + output_filename)

#将所有处理好的表格串成一张表
from google.colab import files
import pandas as pd
from io import BytesIO
import ipywidgets as widgets
from IPython.display import display, clear_output
import time
import os

def merge_files():
    # 创建支持多格式的上传控件
    upload = widgets.FileUpload(
        accept='.xlsx, .xls, .csv',  # 添加CSV支持
        multiple=True,
        description='选择文件'
    )

    # 创建格式说明标签
    format_info = widgets.HTML(
        value="<i>支持格式：Excel(.xlsx/.xls) 和 CSV(.csv)</i>"
    )

    display(format_info, upload)

    out = widgets.Output()
    display(out)

    with out:
        print("请通过上方按钮上传文件（可多选）...")
        while len(upload.value) == 0:
            time.sleep(1)

    try:
        uploaded_files = upload.value
        merged_df = pd.DataFrame()

        with out:
            clear_output()
            print(f"收到 {len(uploaded_files)} 个文件，开始处理...")

            for i, (filename, file_data) in enumerate(uploaded_files.items()):
                # 自动识别文件类型
                ext = os.path.splitext(filename)[1].lower()

                # 进度条
                progress = widgets.FloatProgress(
                    value=(i+1)/len(uploaded_files)*100,
                    description='处理进度:',
                    bar_style='info',
                    orientation='horizontal'
                )
                display(progress)

                # 根据扩展名选择读取方式
                try:
                    if ext in ['.xlsx', '.xls']:
                        df = pd.read_excel(BytesIO(file_data))
                    elif ext == '.csv':
                        # 尝试自动检测编码
                        try:
                            df = pd.read_csv(BytesIO(file_data))
                        except UnicodeDecodeError:
                            df = pd.read_csv(BytesIO(file_data), encoding='gbk')
                    else:
                        raise ValueError(f"不支持的格式: {ext}")
                except Exception as e:
                    print(f"文件 {filename} 读取失败: {str(e)}")
                    continue

                # 合并数据
                merged_df = pd.concat([merged_df, df], axis=0, ignore_index=True, join='outer')

                print(f"已处理: {filename} ({df.shape[0]}行)")
                progress.close()

            # 结果展示
            print("\n合并完成！数据集摘要：")
            print(merged_df.describe(include='all'))

            # 保存合并结果
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            output_file = f"merged_data_{timestamp}.xlsx"
            merged_df.to_excel(output_file, index=False)

            # 添加下载按钮
            download_btn = widgets.Button(description="下载合并文件")
            def on_download_click(b):
                files.download(output_file)
            download_btn.on_click(on_download_click)

            display(download_btn)

    except Exception as e:
        with out:
            print(f"处理失败: {str(e)}")
        raise

merge_files()

from google.colab import files
import pandas as pd
from io import BytesIO
import os
import time

def merge_files_safely():
    print("📂 请选择要合并的文件（支持 .xlsx / .xls / .csv）：")
    uploaded = files.upload()

    merged = pd.DataFrame()

    for name, file in uploaded.items():
        ext = os.path.splitext(name)[1].lower()
        try:
            if ext in ['.xlsx', '.xls']:
                df = pd.read_excel(BytesIO(file))
            elif ext == '.csv':
                try:
                    df = pd.read_csv(BytesIO(file))
                except:
                    df = pd.read_csv(BytesIO(file), encoding='gbk')
            else:
                print(f"⚠️ 跳过不支持的文件：{name}")
                continue

            merged = pd.concat([merged, df], ignore_index=True)
            print(f"✅ 已合并：{name}（{len(df)} 行）")
        except Exception as e:
            print(f"❌ 读取失败：{name} - {e}")

    if merged.empty:
        print("⚠️ 没有成功合并任何文件。")
        return

    filename = f"合并结果_{time.strftime('%Y%m%d_%H%M%S')}.xlsx"
    merged.to_excel(filename, index=False)
    print(f"\n🎉 合并完成！共 {len(merged)} 行，{len(merged.columns)} 列")
    print("📥 正在下载合并结果文件...")
    files.download(filename)

merge_files_safely()

#给'Trending_time_bucket'分组
import pandas as pd
from google.colab import files
from io import BytesIO

# 1. 让用户上传 Excel 文件
uploaded = files.upload()

# 2. 读取上传的文件（只取第一个上传的文件）
for filename in uploaded:
    file = BytesIO(uploaded[filename])
    break

# 3. 读取 Excel 文件到 DataFrame
df = pd.read_excel(file)

# 4. 定义一个函数来根据 'Trending_time_bucket' 分组
def assign_bucket(value):
    try:
        value = str(value).lower().strip().replace('h', '').replace('+', '')
        if '-' in value:
            start, end = map(float, value.split('-'))
        else:
            start = float(value)
            end = start

        if end <= 12:
            return 'bucket1'
        elif 12 < end <= 24:
            return 'bucket2'
        elif 24 < end < 48:
            return 'bucket3'
        elif value = '48'
            return 'bucket4'
    except:
        return 'unknown'

# 5. 应用函数并新建一列
df['bucket_group'] = df['Trending_time_bucket'].apply(assign_bucket)

# 6. 保存为新的Excel文件
output_filename = 'processed_file.xlsx'
df.to_excel(output_filename, index=False)

# 7. 自动下载
files.download(output_filename)