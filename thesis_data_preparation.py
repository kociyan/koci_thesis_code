# -*- coding: utf-8 -*-
"""Thesis_data_preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9BiLhibxkVKNRuQtWS_DPAzSWfHg3hs
"""

#regionåˆ—æ·»åŠ å›½ç±/åœ°åŒºä¿¡æ¯ä½¿ç”¨

# å®‰è£…ä¾èµ–ï¼ˆColabä¸­å¦‚æœå·²å®‰è£…ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰
!pip install pandas openpyxl

import pandas as pd
import os
from google.colab import files

def extract_country_code(filename):
    """æ™ºèƒ½æå–å›½å®¶ä»£ç çš„å¼ºåŒ–ç‰ˆ"""
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    base_name = os.path.splitext(filename)[0]

    # å¸¸è§å›½å®¶ä»£ç æ¨¡å¼è¯†åˆ«
    patterns = [
        r'\b([A-Z]{2})\b',    # åŒ¹é…å…¨å¤§å†™çš„2å­—æ¯ä»£ç 
        r'\b([A-Z]{3})\b',    # åŒ¹é…å…¨å¤§å†™çš„3å­—æ¯ä»£ç 
        r'^([A-Za-z]{2})_',   # å¼€å¤´ä¸¤ä¸ªå­—æ¯+ä¸‹åˆ’çº¿
        r'_([A-Za-z]{2})\b'   # ä¸‹åˆ’çº¿+ç»“å°¾ä¸¤ä¸ªå­—æ¯
    ]

    # ä¼˜å…ˆæ£€æµ‹å·²çŸ¥å¹³å°å‰ç¼€
    platform_prefixes = ['youtube', 'yt', 'video', 'trending']
    for prefix in platform_prefixes:
        if prefix in base_name.lower():
            base_name = base_name.lower().split(prefix)[0]

    # å°è¯•ç”¨åˆ†éš”ç¬¦æ‹†åˆ†
    for sep in ['_', '-', ' ', '#']:
        parts = base_name.split(sep)
        for part in parts:
            clean_part = part.strip().upper()
            if 2 <= len(clean_part) <= 3 and clean_part.isalpha():
                return clean_part

    # æœ€ç»ˆå›é€€æ–¹æ¡ˆ
    return "MODIFIED"

# ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()
original_filename = next(iter(uploaded))
print(f"ğŸ“¤ å·²ä¸Šä¼ æ–‡ä»¶ï¼š{original_filename}")

# æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰
try:
    # Check file extension and use appropriate reader
    if original_filename.lower().endswith('.xlsx'):
        df = pd.read_excel(original_filename)
        print("âœ… ä½¿ç”¨ pd.read_excel æˆåŠŸè¯»å–æ–‡ä»¶")
    else:
        df = pd.read_csv(original_filename, encoding='utf-8-sig')
        print("âœ… ä½¿ç”¨ utf-8-sig ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
except UnicodeDecodeError:
    try:
        df = pd.read_csv(original_filename, encoding='ISO-8859-1')
        print("âœ… ä½¿ç”¨ ISO-8859-1 ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œé”™è¯¯ç±»å‹ï¼š{type(e).__name__}")
        raise

# åˆ—å®šä½åŒä¿é™©æœºåˆ¶
target_col = None

# æ–¹æ¡ˆä¸€ï¼šæŒ‰Jåˆ—ä½ç½®å®šä½ï¼ˆç¬¬10åˆ—ï¼‰
if df.shape[1] > 9:
    position_col = df.columns[9]
    if 'region' in position_col.lower():
        target_col = position_col
        print(f"ğŸ” é€šè¿‡åˆ—ä½ç½®å®šä½åˆ°ç›®æ ‡åˆ—ï¼š{position_col}")

# æ–¹æ¡ˆäºŒï¼šæ™ºèƒ½åç§°åŒ¹é…
if not target_col:
    name_variants = ['region', 'reg', 'countrycode', 'åœ°åŒº']
    for col in df.columns:
        clean_col = col.strip().lower()
        if any(variant in clean_col for variant in name_variants):
            target_col = col
            print(f"ğŸ” é€šè¿‡åç§°åŒ¹é…åˆ°ç›®æ ‡åˆ—ï¼š{col}")
            break

# æ‰§è¡Œæ›¿æ¢æ“ä½œ
if target_col:
    df[target_col] = 'JP'  # æ­¤å¤„ç”¨äºè°ƒæ•´å›½ç±æˆ–åœ°åŒºç¼©å†™

    # ç”Ÿæˆæ ‡å‡†åŒ–æ–‡ä»¶å
    country_code = extract_country_code(original_filename)
    output_filename = f"{country_code}_youtube_trending.csv"

    # ç¡®ä¿CSVæ ¼å¼
    if not output_filename.lower().endswith('.csv'):
        output_filename += '.csv'

    # ä¿å­˜æ–‡ä»¶
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    files.download(output_filename)

    print(f"""
    ğŸ‰ æ“ä½œæˆåŠŸï¼
    â†’ ç›®æ ‡åˆ—ï¼š{target_col}
    â†’ ç”Ÿæˆæ–‡ä»¶ï¼š{output_filename}
    â†’ æ€»å¤„ç†è¡Œæ•°ï¼š{len(df):,}
    """)
else:
    print("""
    â— æœªæ‰¾åˆ°ç›®æ ‡åˆ—ï¼Œå¯èƒ½åŸå› ï¼š
    1. æ•°æ®åˆ—ä½ç½®å‘ç”Ÿå˜åŒ–
    2. åˆ—åä¸åŒ…å«'region'ç›¸å…³å…³é”®è¯
    3. æ–‡ä»¶æ ¼å¼å¼‚å¸¸

    ğŸ” å½“å‰åˆ—ç»“æ„ï¼š
    """)
    print(df.dtypes.to_string())
    print("\nğŸ’¡ è¯·æˆªå›¾å½“å‰æ•°æ®è¡¨ç»“æ„ï¼Œæˆ‘å°†æä¾›å®šåˆ¶è§£å†³æ–¹æ¡ˆ")

#è¿™ä¸ªç”¨æ¥æ‰“æ—¶é—´å·®æ ‡è®°
import pandas as pd
import chardet
from google.colab import files

# æ£€æµ‹æ–‡ä»¶ç¼–ç 
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        raw_data = f.read(100000)
    bom_encodings = {b'\xff\xfe': 'utf-16-le', b'\xfe\xff': 'utf-16-be', b'\xef\xbb\xbf': 'utf-8-sig'}
    for bom, encoding in bom_encodings.items():
        if raw_data.startswith(bom):
            return encoding
    result = chardet.detect(raw_data)
    return result['encoding'] if result['confidence'] > 0.8 else None

# è¯»å–CSVæ–‡ä»¶ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç¼ºå¤±å€¼
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'gb18030', 'cp1252']
    if encoding and encoding not in encodings_to_try:
        encodings_to_try.insert(0, encoding)
    for codec in encodings_to_try:
        try:
            return pd.read_csv(file_path, encoding=codec, engine='python', dtype=str, keep_default_na=False, na_values=[''])
        except Exception:
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

df['publishedAt'] = pd.to_datetime(df['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')
df['trending_date'] = pd.to_datetime(df['trending_date'], format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')

df['time_diff_hours'] = (df['trending_date'] - df['publishedAt']).dt.total_seconds() / 3600

def categorize_time_diff(hours):
    if pd.isna(hours):
        return 'ä¸å¯ç”¨'
    if 0 <= hours <= 12:
        return '12h'
    elif 12 < hours <= 24:
        return '12-24h'
    elif 24 < hours <= 48:
        return '24-48h'
    elif hours > 48:
        return '48+h'
    else:
        return 'ä¸å¯ç”¨'

df['æ—¶é—´å·®åˆ†ç±»'] = df['time_diff_hours'].apply(categorize_time_diff)

# è¿˜åŸ publishedAt å’Œ trending_date çš„æ ¼å¼
df['publishedAt'] = df['publishedAt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = df['trending_date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# ç¡®ä¿ video_id ä»¥å­—ç¬¦ä¸²æ–¹å¼å¤„ç†ï¼Œå¹¶æ­£ç¡®è¯†åˆ«æ‰€æœ‰å†…å®¹
df['video_id'] = df['video_id'].astype(str).fillna('UNKNOWN')
df['ç»„å·'] = df.groupby(df['video_id'], sort=False, dropna=False, group_keys=False).ngroup() + 1

# å®šä¹‰æ’åºä¼˜å…ˆçº§
time_diff_order = {'12h': 1, '12-24h': 2, '24-48h': 3, '48+h': 4, 'ä¸å¯ç”¨': 5}
df['æ—¶é—´å·®æ’åº'] = df['æ—¶é—´å·®åˆ†ç±»'].map(time_diff_order)

# å…ˆæŒ‰ç»„å·æ’åºï¼Œå†æŒ‰æ—¶é—´å·®åˆ†ç±»æ’åº
df.sort_values(by=['ç»„å·', 'æ—¶é—´å·®æ’åº'], inplace=True)

# åˆ é™¤ä¸´æ—¶æ’åºåˆ—
df.drop(columns=['æ—¶é—´å·®æ’åº'], inplace=True)

output_filename = f"processed_{input_filename}"
df.to_csv(output_filename, index=False, encoding='utf-8-sig')

print("\nâœ… å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¸‹è½½æ–‡ä»¶...")
files.download(output_filename)
print(df[['video_id', 'ç»„å·', 'publishedAt', 'trending_date', 'time_diff_hours', 'æ—¶é—´å·®åˆ†ç±»']].head())

#å¤„ç†æŒ‰ç…§video_idåˆ†ç»„çš„é—®é¢˜ï¼Œä½†æ˜¯è¿™ä»½ä»£ç ä¸èƒ½å‡†ç¡®è¯†åˆ«video_idï¼Œæœ‰å¾ˆå¤šNAME?é”™è¯¯å­˜åœ¨ã€‚
import pandas as pd
import chardet
import csv
from google.colab import files

# å¢å¼ºç‰ˆç¼–ç æ£€æµ‹ï¼ˆå¤„ç†BOMå¤´ï¼‰
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # å¼ºåˆ¶æŒ‡å®šä¸ºå­—ç¬¦ä¸²ç±»å‹
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# æ¸…ç†video_idåˆ—ï¼ˆå¤„ç†Excelå…¬å¼æ³¨å…¥é—®é¢˜ï¼‰
df['video_id'] = (
    df['video_id']
    .astype(str)
    .str.strip()
    .str.replace(r'^=', "'=", regex=True)  # åœ¨ç­‰å·å‰æ·»åŠ å•å¼•å·é˜²æ­¢Excelè§£æ
    .str.replace(r'\x00', '', regex=True)    # ç§»é™¤ç©ºå­—ç¬¦
)

# æ—¥æœŸå¤„ç†ï¼ˆå¸¦é”™è¯¯æ£€æŸ¥ï¼‰
def parse_datetime(col):
    try:
        return pd.to_datetime(col, format='%Y-%m-%dT%H:%M:%SZ', utc=True, errors='coerce')
    except Exception as e:
        print(f"æ—¥æœŸè§£æé”™è¯¯ï¼š{str(e)}")
        return pd.NaT

df['publishedAt_dt'] = parse_datetime(df['publishedAt'])
df['trending_date_dt'] = parse_datetime(df['trending_date'])

# æ—¶é—´å·®è®¡ç®—ï¼ˆå°æ—¶ï¼‰
df['time_diff_hours'] = (
    (df['trending_date_dt'] - df['publishedAt_dt'])
    .dt.total_seconds()
    .div(3600)
    .round(2)
)

# æ—¶é—´å·®åˆ†ç±»é€»è¾‘
def categorize_time_diff(hours):
    if pd.isna(hours):
        return 'ä¸å¯ç”¨'
    if 0 <= hours <= 12:
        return '12h'
    elif 12 < hours <= 24:
        return '12-24h'
    elif 24 < hours <= 48:
        return '24-48h'
    elif hours > 48:
        return '48+h'
    else:
        return 'ä¸å¯ç”¨'

df['æ—¶é—´å·®åˆ†ç±»'] = df['time_diff_hours'].apply(categorize_time_diff)

# åˆ†ç»„é€»è¾‘ï¼ˆç¡®ä¿åŒä¸€video_idä¿æŒç›¸åŒç»„å·ï¼‰
df['ç»„å·'] = (
    df.groupby('video_id', sort=False, dropna=False)
    .ngroup()
    .add(1)
    .astype(str)
    .str.zfill(4)  # ç”Ÿæˆ4ä½æ•°å­—ç»„å·ï¼Œå¦‚0001
)

# æ’åºé€»è¾‘ï¼ˆå¸¦ä¼˜å…ˆçº§ï¼‰
time_diff_order = {'12h': 1, '12-24h': 2, '24-48h': 3, '48+h': 4, 'ä¸å¯ç”¨': 5}
df['_æ—¶é—´å·®æ’åº'] = df['æ—¶é—´å·®åˆ†ç±»'].map(time_diff_order)

# æœ€ç»ˆæ’åºï¼ˆç»„å· > æ—¶é—´å·®åˆ†ç±» > å‘å¸ƒæ—¶é—´ï¼‰
df = df.sort_values(
    by=['ç»„å·', '_æ—¶é—´å·®æ’åº', 'publishedAt_dt'],
    ascending=[True, True, True]
).drop(columns=['_æ—¶é—´å·®æ’åº', 'publishedAt_dt', 'trending_date_dt'])

# æ¢å¤åŸå§‹æ—¥æœŸæ ¼å¼
df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = pd.to_datetime(df['trending_date']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# å¯¼å‡ºè®¾ç½®ï¼ˆé˜²æ­¢Excelå…¬å¼æ³¨å…¥ï¼‰
output_filename = f"processed_{input_filename}"
df.to_csv(
    output_filename,
    index=False,
    encoding='utf-8-sig',
    quoting=csv.QUOTE_ALL,  # æ‰€æœ‰å­—æ®µåŠ å¼•å·
    escapechar='\\',        # å¤„ç†åŒ…å«å¼•å·çš„æƒ…å†µ
    date_format='%Y-%m-%dT%H:%M:%SZ'
)

print("\nâœ… å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¸‹è½½æ–‡ä»¶...")
files.download(output_filename)

# æ˜¾ç¤ºç¤ºä¾‹æ•°æ®ï¼ˆéªŒè¯å¤„ç†ç»“æœï¼‰
print("\nå¤„ç†ç»“æœç¤ºä¾‹ï¼š")
display(df[['video_id', 'ç»„å·', 'publishedAt', 'trending_date', 'time_diff_hours', 'æ—¶é—´å·®åˆ†ç±»']].head(3))

import pandas as pd
import chardet
import csv
import re
from google.colab import files

# å¢å¼ºç‰ˆç¼–ç æ£€æµ‹ï¼ˆå¤„ç†BOMå¤´ï¼‰
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# å®‰å…¨è¯»å–CSVï¼ˆå¼ºåŒ–ç±»å‹æŒ‡å®šï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': 'string'},  # ä½¿ç”¨pandasçš„stringç±»å‹
                keep_default_na=False,
                na_filter=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"â›” è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# æ ¸å¿ƒIDæ¸…æ´—å‡½æ•°
def sanitize_video_id(series):
    return (
        series
        .astype(str)  # Convert the column to string explicitly
        .str.strip()
        # å¤„ç†å…¬å¼æ³¨å…¥é£é™©
        .replace(r'^[=+\-@]', lambda x: f"'{x.group(0) if x.group(0) is not None else ''}", regex=True)  # Handle potential empty matches
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        .str.replace(r'[\x00-\x1F\x7F]', '', regex=True)
        # å¤„ç†è¶…é•¿ID
        .apply(lambda x: f"'{x}" if len(str(x)) > 15 and x.isdigit() else x)
    )

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# æ‰§è¡ŒIDæ¸…æ´—ï¼ˆå¸¦æ—¥å¿—è®°å½•ï¼‰
original_ids = df['video_id'].copy()
df['video_id'] = sanitize_video_id(df['video_id'])

# å¯¹æ¯”æ£€æŸ¥æ¸…æ´—ç»“æœ
changed_mask = original_ids != df['video_id']
if changed_mask.any():
    print("\nğŸ” æ£€æµ‹åˆ°ä»¥ä¸‹video_idè¢«ä¿®æ”¹ï¼š")
    display(pd.concat([
        original_ids[changed_mask].rename('åŸå§‹ID'),
        df['video_id'][changed_mask].rename('å¤„ç†åID')
    ], axis=1).head(5))

# æ—¥æœŸå¤„ç†ï¼ˆå¢å¼ºå®¹é”™ï¼‰
def safe_datetime_parse(s, format):
    try:
        return pd.to_datetime(s, format=format, utc=True)
    except Exception as e:
        print(f"âš ï¸ æ—¥æœŸè§£æé”™è¯¯ï¼š{e}\næ ·ä¾‹å€¼ï¼š{s.head(1).values}")
        return pd.to_datetime(s, errors='coerce', utc=True)

df['publishedAt_dt'] = safe_datetime_parse(df['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')
df['trending_date_dt'] = safe_datetime_parse(df['trending_date'], '%Y-%m-%dT%H:%M:%SZ')

# æ—¶é—´å·®è®¡ç®—ï¼ˆç²¾ç¡®åˆ°åˆ†é’Ÿï¼‰
df['time_diff'] = (df['trending_date_dt'] - df['publishedAt_dt']).dt.total_seconds() / 60
df['time_diff_hours'] = (df['time_diff'] / 60).round(2)

# æ—¶é—´å·®åˆ†ç±»ï¼ˆåŠ¨æ€åŒºé—´ï¼‰
bins = [0, 12, 24, 48, float('inf')]
labels = ['12h', '12-24h', '24-48h', '48+h']
df['æ—¶é—´å·®åˆ†ç±»'] = pd.cut(
    df['time_diff_hours'],
    bins=bins,
    labels=labels,
    right=False,
    include_lowest=True
).cat.add_categories('ä¸å¯ç”¨').fillna('ä¸å¯ç”¨')

# åˆ†ç»„é€»è¾‘ï¼ˆç¨³å®šå“ˆå¸Œåˆ†ç»„ï¼‰
df['ç»„å·'] = (
    df.groupby('video_id', sort=False, dropna=False)
    .ngroup()
    .add(1)
    .astype(str)
    .str.zfill(4)
)

# æ’åºé€»è¾‘ï¼ˆå¤šçº§æ’åºï¼‰
df = df.sort_values(
    by=['ç»„å·', 'time_diff_hours', 'publishedAt_dt'],
    ascending=[True, True, True]
)

# è¿˜åŸåŸå§‹æ—¥æœŸæ ¼å¼
df['publishedAt'] = df['publishedAt_dt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')
df['trending_date'] = df['trending_date_dt'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# æ¸…ç†ä¸­é—´åˆ—
df = df.drop(columns=['publishedAt_dt', 'trending_date_dt', 'time_diff'])

# å¯¼å‡ºé…ç½®ï¼ˆExcelå…¼å®¹æ ¼å¼ï¼‰
output_filename = f"processed_{input_filename}"
df.to_csv(
    output_filename,
    index=False,
    encoding='utf-8-sig',
    quoting=csv.QUOTE_ALL,
    escapechar='\\',
    date_format='%Y-%m-%dT%H:%M:%SZ',
    # newline='\r\n'  # Remove or comment out this line
)

print("\nâœ… å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¸‹è½½æ–‡ä»¶...")
files.download(output_filename)

# æœ€ç»ˆéªŒè¯
print("\nğŸ” æœ€ç»ˆæ•°æ®éªŒè¯ï¼š")
print(f"æ€»è®°å½•æ•°ï¼š{len(df)}")
print("video_id æ ·ä¾‹ï¼š")
print(df['video_id'].sample(5).to_string(index=False))

from google.colab import files
import pandas as pd
from tqdm import tqdm
import io

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()
file_name = next(iter(uploaded))
df = pd.read_csv(io.BytesIO(uploaded[file_name]))  # å…³é”®ä¿®æ”¹ç‚¹

# æ„å»º channelId åˆ°æœ‰æ•ˆ video_id çš„æ˜ å°„
print("\næ­¥éª¤ 1/2: åˆ†æ channelId å¯¹åº”çš„æœ‰æ•ˆ video_id...")
channel_id_map = {}
valid_channels = df[df['channelId'] != '#NAME?']
unique_channel_ids = valid_channels['channelId'].unique()

for channel_id in tqdm(unique_channel_ids, desc="åˆ†æè¿›åº¦"):
    video_ids = valid_channels[valid_channels['channelId'] == channel_id]['video_id']
    video_ids_clean = video_ids[video_ids != '#NAME?']

    if video_ids_clean.nunique() == 1:
        channel_id_map[channel_id] = video_ids_clean.iloc[0]
    else:
        channel_id_map[channel_id] = None

# æ›¿æ¢æ— æ•ˆçš„ video_id
print("\næ­¥éª¤ 2/2: ä¿®å¤æ— æ•ˆçš„ video_id...")
need_fix_mask = (df['video_id'] == '#NAME?') & (df['channelId'].isin(channel_id_map))
to_fix_indices = df[need_fix_mask].index.tolist()

for idx in tqdm(to_fix_indices, desc="ä¿®å¤è¿›åº¦"):
    channel_id = df.loc[idx, 'channelId']
    if channel_id_map.get(channel_id):
        df.loc[idx, 'video_id'] = channel_id_map[channel_id]

# ä¿å­˜å¹¶ä¸‹è½½ç»“æœï¼ˆä¿æŒCSVæ ¼å¼ï¼‰
output_filename = f"processed_{file_name}"
df.to_csv(output_filename, index=False)
files.download(output_filename)

print("\nå¤„ç†å®Œæˆï¼æ›´æ–°åçš„æ–‡ä»¶å·²è‡ªåŠ¨ä¸‹è½½ã€‚")

#é€šè¿‡channelidå’ŒpublishatåæŸ¥video_id,è§£å†³éƒ¨åˆ†çš„#NAME?é—®é¢˜
import pandas as pd
import numpy as np
import tqdm
from google.colab import files
from io import BytesIO
from tqdm.notebook import tqdm

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è¯»å–
if filename.endswith('.csv'):
    df = pd.read_csv(BytesIO(uploaded[filename]))  # è¯»å– CSV
else:
    df = pd.read_excel(BytesIO(uploaded[filename]), engine='openpyxl')  # è¯»å– Excelï¼ˆå…¼å®¹ .xlsxï¼‰

# è¿›åº¦æ¡åˆå§‹åŒ–
tqdm.pandas()

def replace_name(row, df):
    if row['video_id'] == '#NAME?':
        channel_id = row['channelId']
        if channel_id != '#NAME?':
            # æŸ¥æ‰¾ç›¸åŒ channelId çš„æ‰€æœ‰è¡Œ
            matching_rows = df[df['channelId'] == channel_id]
            unique_video_ids = matching_rows['video_id'].unique()

            # ç§»é™¤ '#NAME?'
            unique_video_ids = [vid for vid in unique_video_ids if vid != '#NAME?']

            if len(unique_video_ids) == 1:
                return unique_video_ids[0]  # åªæœ‰ä¸€ä¸ªå”¯ä¸€çš„ video_id
            else:
                # æŸ¥æ‰¾ publishedAt æ˜¯å¦æœ‰é‡å¤çš„
                duplicate_published = matching_rows[matching_rows.duplicated('publishedAt', keep=False)]
                if not duplicate_published.empty:
                    return duplicate_published.iloc[0]['video_id']  # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ video_id
    return row['video_id']

# å¤„ç†æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦
df['video_id'] = df.progress_apply(lambda row: replace_name(row, df), axis=1)

# ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
output_filename = "processed_file.xlsx"
df.to_excel(output_filename, index=False, engine='openpyxl')

# è‡ªåŠ¨ä¸‹è½½
files.download(output_filename)
print("å¤„ç†å®Œæˆï¼Œæ–‡ä»¶å·²è‡ªåŠ¨ä¸‹è½½ï¼")

#åˆ æ‰æ‰€æœ‰çš„'ä¸å¯ç”¨'
import pandas as pd
import numpy as np
from google.colab import files

# å¢å¼ºç‰ˆç¼–ç æ£€æµ‹ï¼ˆå¤„ç†BOMå¤´ï¼‰
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # å¼ºåˆ¶æŒ‡å®šä¸ºå­—ç¬¦ä¸²ç±»å‹
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. åˆ é™¤â€œæ—¶é—´å·®åˆ†ç±»â€åˆ—ä¸­å€¼ä¸ºâ€œä¸å¯ç”¨â€çš„è¡Œ
df = df[df['æ—¶é—´å·®åˆ†ç±»'] != 'ä¸å¯ç”¨']

# 2. æŒ‰ç…§ video_id åˆ†ç»„ï¼Œå¹¶æŒ‰ç…§ time_diff_hours è¿›è¡Œæ’åº
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['ç»„å·'] = np.nan  # åˆå§‹åŒ–ç»„å·åˆ—

group_id = 1  # åˆå§‹ç»„å·
group_mapping = {}  # å­˜å‚¨ video_id å¯¹åº”çš„ç»„å·

# éå†æ•°æ®ï¼ŒæŒ‰ video_id èµ‹äºˆç»„å·
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, 'ç»„å·'] = group_mapping[vid]

# 3. å¤„ç† video_id ä¸º â€œ#NAME?â€ çš„æƒ…å†µ
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # è®¡ç®—æ­£å¸¸çš„ç»„æ•°

df.loc[name_mask, 'ç»„å·'] = np.arange(num_existing_groups + 1, num_existing_groups + 1 + name_mask.sum())

# 4. é‡æ–°æ’åºï¼Œç¡®ä¿ #NAME? çš„æ•°æ®åœ¨æœ€å
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

#æŒ‰ç…§(trend-publish)timeæ’åˆ—çš„video_idé‡æ–°åˆ†ç»„,å¹¶å°†æ‰€æœ‰çš„NAME?å•ç‹¬åˆ’ç»„
import pandas as pd
import numpy as np
from google.colab import files

# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # å¼ºåˆ¶æŒ‡å®šä¸ºå­—ç¬¦ä¸²ç±»å‹
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. åˆ é™¤â€œæ—¶é—´å·®åˆ†ç±»â€åˆ—ä¸­å€¼ä¸ºâ€œä¸å¯ç”¨â€çš„è¡Œ
#initial_row_count = len(df)
#df = df[df['æ—¶é—´å·®åˆ†ç±»'] != 'ä¸å¯ç”¨']
#deleted_rows = initial_row_count - len(df)
#print(f"åˆ é™¤çš„è¡Œæ•°: {deleted_rows}")

# 2. æŒ‰ç…§ video_id åˆ†ç»„ï¼Œå¹¶æŒ‰ç…§ time_diff_hours è¿›è¡Œæ’åº
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['ç»„å·'] = np.nan  # åˆå§‹åŒ–ç»„å·åˆ—

group_id = 1  # åˆå§‹ç»„å·
group_mapping = {}  # å­˜å‚¨ video_id å¯¹åº”çš„ç»„å·

# éå†æ•°æ®ï¼ŒæŒ‰ video_id èµ‹äºˆç»„å·
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, 'ç»„å·'] = group_mapping[vid]

# 3. å¤„ç† video_id ä¸º â€œ#NAME?â€ çš„æƒ…å†µ
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # è®¡ç®—æ­£å¸¸çš„ç»„æ•°

# ä¸ºæ¯ä¸ª "#NAME?" å•ç‹¬åˆ†é…ä¸€ä¸ªç»„å·
name_indices = df.index[name_mask]
for i, idx in enumerate(name_indices):
    df.at[idx, 'ç»„å·'] = num_existing_groups + 1 + i

# 4. é‡æ–°æ’åºï¼Œç¡®ä¿ #NAME? çš„æ•°æ®åœ¨æœ€å
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# è¾“å‡ºåˆ†ç»„æ•°
total_groups = df['ç»„å·'].nunique()
print(f"æ€»åˆ†ç»„æ•°: {total_groups}")

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

#æŒ‰ç…§(trend-publish)timeæ’åˆ—çš„video_idé‡æ–°åˆ†ç»„,å¹¶å°†æ‰€æœ‰çš„NAME?å•ç‹¬åˆ’ç»„
import pandas as pd
import numpy as np
from google.colab import files
import chardet # Added import statement
import csv
# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def detect_file_encoding(file_path):
    with open(file_path, 'rb') as f:
        bom = f.read(4)
        bom_encodings = {
            b'\xff\xfe': 'utf-16-le',
            b'\xfe\xff': 'utf-16-be',
            b'\xef\xbb\xbf': 'utf-8-sig'
        }
        for signature, encoding in bom_encodings.items():
            if bom.startswith(signature):
                return encoding
        raw_data = bom + f.read(100000)
        result = chardet.detect(raw_data)
        return result['encoding'] if result['confidence'] > 0.8 else None

# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # å¼ºåˆ¶æŒ‡å®šä¸ºå­—ç¬¦ä¸²ç±»å‹
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 1. åˆ é™¤â€œæ—¶é—´å·®åˆ†ç±»â€åˆ—ä¸­å€¼ä¸ºâ€œä¸å¯ç”¨â€çš„è¡Œ
#initial_row_count = len(df)
#df = df[df['æ—¶é—´å·®åˆ†ç±»'] != 'ä¸å¯ç”¨']
#deleted_rows = initial_row_count - len(df)
#print(f"åˆ é™¤çš„è¡Œæ•°: {deleted_rows}")

# 2. æŒ‰ç…§ video_id åˆ†ç»„ï¼Œå¹¶æŒ‰ç…§ time_diff_hours è¿›è¡Œæ’åº
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['ç»„å·'] = np.nan  # åˆå§‹åŒ–ç»„å·åˆ—

group_id = 1  # åˆå§‹ç»„å·
group_mapping = {}  # å­˜å‚¨ video_id å¯¹åº”çš„ç»„å·

# éå†æ•°æ®ï¼ŒæŒ‰ video_id èµ‹äºˆç»„å·
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, 'ç»„å·'] = group_mapping[vid]

# 3. å¤„ç† video_id ä¸º â€œ#NAME?â€ çš„æƒ…å†µ
name_mask = df['video_id'] == '#NAME?'
num_existing_groups = len(group_mapping)  # è®¡ç®—æ­£å¸¸çš„ç»„æ•°

# ä¸ºæ¯ä¸ª "#NAME?" å•ç‹¬åˆ†é…ä¸€ä¸ªç»„å·
name_indices = df.index[name_mask]
for i, idx in enumerate(name_indices):
    df.at[idx, 'ç»„å·'] = num_existing_groups + 1 + i

# 4. é‡æ–°æ’åºï¼Œç¡®ä¿ #NAME? çš„æ•°æ®åœ¨æœ€å
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# è¾“å‡ºåˆ†ç»„æ•°
total_groups = df['ç»„å·'].nunique()
print(f"æ€»åˆ†ç»„æ•°: {total_groups}")

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

#ä¸Šé¢çš„ä»£ç è¿˜æ˜¯ä¸è¡Œï¼Œå†åº¦ä¿®å¤
import pandas as pd
import numpy as np
from google.colab import files

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# 1. åˆ é™¤â€œæ—¶é—´å·®åˆ†ç±»â€åˆ—ä¸­å€¼ä¸ºâ€œä¸å¯ç”¨â€çš„è¡Œ
initial_row_count = len(df)
df = df[df['æ—¶é—´å·®åˆ†ç±»'] != 'ä¸å¯ç”¨']
deleted_rows = initial_row_count - len(df)
print(f"åˆ é™¤çš„è¡Œæ•°: {deleted_rows}")

# 2. å¤„ç† video_id ä¸º â€œ#NAME?â€ çš„æƒ…å†µ
name_mask = df['video_id'] == '#NAME?'
name_indices = df.index[name_mask]

# ç›´æ¥ä¿®æ”¹ video_idï¼Œç¡®ä¿æ¯ä¸€è¡Œå”¯ä¸€
df.loc[name_indices, 'video_id'] = [f"#NAME? {i+1}" for i in range(len(name_indices))]

# 3. æŒ‰ç…§ video_id åˆ†ç»„ï¼Œå¹¶æŒ‰ç…§ time_diff_hours è¿›è¡Œæ’åº
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)

df['ç»„å·'] = np.nan  # åˆå§‹åŒ–ç»„å·åˆ—

group_id = 1  # åˆå§‹ç»„å·
group_mapping = {}  # å­˜å‚¨ video_id å¯¹åº”çš„ç»„å·

# éå†æ•°æ®ï¼ŒæŒ‰ video_id èµ‹äºˆç»„å·
for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, 'ç»„å·'] = group_mapping[vid]

# 4. é‡æ–°æ’åºï¼Œç¡®ä¿ #NAME? çš„æ•°æ®åœ¨æœ€å
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# è¾“å‡ºåˆ†ç»„æ•°
total_groups = df['ç»„å·'].nunique()
print(f"æ€»åˆ†ç»„æ•°: {total_groups}")

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

import pandas as pd
from google.colab import files

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# è¯»å–æ•°æ®å¹¶é¢„å¤„ç†
df = pd.read_excel(filename)
df = df[df['æ—¶é—´å·®åˆ†ç±»'] != 'ä¸å¯ç”¨']

# åˆ†ç¦»æ­£å¸¸æ•°æ®å’Œå¼‚å¸¸æ•°æ®
normal_df = df[df['video_id'] != '#NAME?'].copy()
name_df = df[df['video_id'] == '#NAME?'].copy()

# å¤„ç†æ­£å¸¸æ•°æ®çš„åˆ†ç»„
if not normal_df.empty:
    # æŒ‰video_idåˆ†ç»„å¹¶æ’åº
    normal_df = normal_df.sort_values(['video_id', 'time_diff_hours'])
    # åˆ†é…ç»„å·ï¼ˆä»1å¼€å§‹ï¼‰
    normal_df['ç»„å·'] = normal_df.groupby('video_id').ngroup() + 1
else:
    normal_df['ç»„å·'] = pd.NA

# å¤„ç†å¼‚å¸¸æ•°æ®çš„åˆ†ç»„
if not name_df.empty:
    # ç”Ÿæˆå”¯ä¸€video_idï¼ˆrename1, rename2...ï¼‰
    name_df = name_df.sort_values('time_diff_hours')
    name_df['video_id'] = [f'rename{i+1}' for i in range(len(name_df))]

    # è·å–æœ€å¤§å·²æœ‰ç»„å·
    max_group = normal_df['ç»„å·'].max() if not normal_df.empty else 0

    # ä¸ºæ¯ä¸ªå¼‚å¸¸è¡Œåˆ†é…ç‹¬ç«‹ç»„å·
    name_df['ç»„å·'] = range(max_group + 1, max_group + 1 + len(name_df))

# åˆå¹¶æ•°æ®å¹¶æœ€ç»ˆæ’åº
final_df = pd.concat([normal_df, name_df]).sort_values(
    ['ç»„å·', 'time_diff_hours'],
    ignore_index=True
)

# éªŒè¯è¾“å‡º
print(f"æ€»åˆ†ç»„æ•°ï¼š{final_df['ç»„å·'].nunique()}")
print("\nå‰5ä¸ªæ­£å¸¸ç»„æ ·ä¾‹ï¼š")
print(final_df[final_df['ç»„å·'] <= max_group].head())
print("\nå¼‚å¸¸ç»„æ ·ä¾‹ï¼š")
print(final_df[final_df['ç»„å·'] > max_group].head())

# å¯¼å‡ºæ–‡ä»¶
output_filename = 'processed_data_v3.xlsx'
final_df.to_excel(output_filename, index=False)
files.download(output_filename)

#æ¯æ¬¡éƒ½è¦æ‰“å¼€æ–‡ä»¶æ‰‹åŠ¨è§£å†³NAME?çš„åˆ†ç»„é—®é¢˜ï¼Œè¿™ä¸ªå§‹ç»ˆæ— æ³•é€šè¿‡ä»£ç å®ç°ã€‚

#æŸ¥æ‰¾renameè¡Œä¸­Bä¸CåŒæ—¶é‡å¤çš„å€¼ï¼Œå¹¶å°†å®ƒä»¬é‡æ–°åˆ†ç»„
import pandas as pd
import numpy as np
from google.colab import files

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# 1. ç­›é€‰å‡º video_id å«æœ‰ "rename" çš„è¡Œ
rename_mask = df['video_id'].astype(str).str.contains("rename", case=False, na=False)
rename_df = df[rename_mask].copy()

# 2. è¯†åˆ« publishedAt å’Œ channelId åŒæ—¶é‡å¤çš„è¡Œ
grouped = rename_df.groupby(['publishedAt', 'channelId'])
fix_id = 1  # fix ç»„è®¡æ•°
fix_mapping = {}  # å­˜å‚¨ç›¸åŒ publishedAt-channelId ç»„åˆçš„ç»„å·

for _, group in grouped:
    if len(group) > 1:  # ä»…å¤„ç†é‡å¤é¡¹
        group_indices = group.index
        fix_label = f"fix{fix_id}"
        rename_df.loc[group_indices, 'video_id'] = fix_label  # ä¿®æ”¹ video_id
        fix_mapping[fix_label] = fix_id  # è®°å½•ç»„å·
        fix_id += 1

# 3. é‡æ–°åˆ†é…ç»„å·
rename_df['ç»„å·'] = np.nan  # é‡æ–°åˆå§‹åŒ–ç»„å·åˆ—
rename_groups = rename_df['video_id'].unique()
group_id = 1

group_dict = {}  # å­˜å‚¨ group_id å¯¹åº”çš„ç»„å·
for vid in rename_groups:
    group_dict[vid] = group_id
    group_id += 1

rename_df['ç»„å·'] = rename_df['video_id'].map(group_dict)

# 4. ç¡®ä¿ fix ç»„å·æ’åœ¨ rename ç»„å·ä¹‹å
df.update(rename_df)  # æ›´æ–°åŸ DataFrame
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼Œæ–‡ä»¶å·²ç”Ÿæˆå¹¶å¯ä¸‹è½½ã€‚")

#é‡æ–°å¤„ç†åˆ†ç»„é—®é¢˜
import pandas as pd
import numpy as np
from google.colab import files

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# 1. ç­›é€‰å‡º video_id å«æœ‰ "rename" çš„è¡Œ
rename_mask = df['video_id'].astype(str).str.contains("rename", case=False, na=False)
rename_df = df[rename_mask].copy()

# 2. è¯†åˆ« publishedAt å’Œ channelId åŒæ—¶é‡å¤çš„è¡Œ
grouped = rename_df.groupby(['publishedAt', 'channelId'])
fix_id = 1  # fix ç»„è®¡æ•°
fix_mapping = {}  # å­˜å‚¨ç›¸åŒ publishedAt-channelId ç»„åˆçš„ç»„å·

for _, group in grouped:
    if len(group) > 1:  # ä»…å¤„ç†é‡å¤é¡¹
        group_indices = group.index
        fix_label = f"fix{fix_id}"
        rename_df.loc[group_indices, 'video_id'] = fix_label  # ä¿®æ”¹ video_id
        fix_mapping[fix_label] = fix_id  # è®°å½•ç»„å·
        fix_id += 1

# 3. é‡æ–°åˆ†é…ç»„å·
rename_df['ç»„å·'] = np.nan  # é‡æ–°åˆå§‹åŒ–ç»„å·åˆ—
rename_groups = rename_df['video_id'].unique()
existing_max_group_id = df['ç»„å·'].max() if pd.notna(df['ç»„å·']).any() else 0  # æ‰¾åˆ°å½“å‰æœ€å¤§ç»„å·

group_id = existing_max_group_id + 1  # fix ç»„å·åº”æ’åœ¨æ‰€æœ‰ç°æœ‰ç»„å·ä¹‹å
fix_group_dict = {}  # å­˜å‚¨ fix ç»„çš„ç»„å·

for vid in rename_groups:
    fix_group_dict[vid] = group_id
    group_id += 1

rename_df['ç»„å·'] = rename_df['video_id'].map(fix_group_dict)

df.update(rename_df)  # æ›´æ–°åŸ DataFrame

# 4. é‡æ–°æ‰§è¡Œ video_id å½’ç»„ï¼Œç¡®ä¿æ‰€æœ‰ç›¸åŒ video_id å½’ä¸ºåŒä¸€ç»„
df.sort_values(by=['video_id', 'time_diff_hours'], ascending=[True, True], inplace=True)
df['ç»„å·'] = np.nan  # é‡æ–°åˆå§‹åŒ–ç»„å·

group_id = 1  # é‡æ–°è®¡ç®—ç»„å·
group_mapping = {}  # å­˜å‚¨ video_id å¯¹åº”çš„ç»„å·

for index, row in df.iterrows():
    vid = row['video_id']
    if vid not in group_mapping:
        group_mapping[vid] = group_id
        group_id += 1
    df.at[index, 'ç»„å·'] = group_mapping[vid]

# 5. ç¡®ä¿ fix ç»„å·æ’åœ¨æ‰€æœ‰å…¶ä»–ç»„å·ä¹‹å
df.sort_values(by=['ç»„å·', 'time_diff_hours'], inplace=True)

# å¯¼å‡ºå¤„ç†åçš„ Excel æ–‡ä»¶
output_filename = 'processed_data.xlsx'
df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼Œæ–‡ä»¶å·²ç”Ÿæˆå¹¶å¯ä¸‹è½½ã€‚")

#localtimeè½¬æ¢æœ‰æ•ˆ

# å®‰è£…ä¾èµ–åŒ…ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
!pip install pytz pandas

# --- å®Œæ•´ä»£ç å¼€å§‹ ---
import pandas as pd
import pytz
from google.colab import files

# å®‰å…¨è¯»å–CSVï¼ˆå¢å¼ºå¯¹ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ï¼‰
def read_csv_safe(file_path):
    encoding = detect_file_encoding(file_path)
    encodings_to_try = ['utf-8-sig', 'utf-8', 'gb18030', 'latin1', 'cp1252']

    for codec in encodings_to_try + ([encoding] if encoding else []):
        try:
            return pd.read_csv(
                file_path,
                encoding=codec,
                dtype={'video_id': str},  # å¼ºåˆ¶æŒ‡å®šä¸ºå­—ç¬¦ä¸²ç±»å‹
                keep_default_na=False,
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL,
                engine='python',
                on_bad_lines='warn'
            )
        except UnicodeDecodeError:
            continue
        except pd.errors.ParserError as e:
            print(f"è§£æé”™è¯¯ï¼ˆç¼–ç  {codec}ï¼‰: {str(e)}")
            continue
    raise ValueError("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")

# ä¸Šä¼ æ–‡ä»¶
print("ğŸ”¼ è¯·ä¸Šä¼ CSVæ–‡ä»¶ï¼š")
uploaded = files.upload()
input_filename = next(iter(uploaded))
df = read_csv_safe(input_filename)

# 3. è§£æUTCæ—¶é—´åˆ—ä¸ºæ—¶åŒºæ•æ„Ÿå¯¹è±¡
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. å®šä¹‰æ—¶åŒºè½¬æ¢å‡½æ•°
def get_timezone(region_code):
    try:
        return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°å›½å®¶ä»£ç  '{region_code}' çš„æ—¶åŒº")
        return None

# 5. è½¬æ¢æ—¶é—´å¹¶åˆ†å‰²ä¸ºæ—¥æœŸå’Œæ—¶é—´åˆ—
def convert_and_split_time(row):
    tz = get_timezone(row["region"])
    if not tz:
        return pd.NaT, pd.NaT

    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´å¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # åˆ†å‰²ä¸ºæ—¥æœŸå’Œæ—¶é—´å­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD å’Œ HH:MM:SSï¼‰
    return local_time.date().isoformat(), local_time.time().strftime("%H:%M:%S")

# åº”ç”¨è½¬æ¢å‡½æ•°ç”Ÿæˆä¸¤åˆ—
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 6. æ¸…ç†åŸå§‹UTCæ—¶é—´åˆ—çš„æ—¶åŒºä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 7. æ˜¾ç¤ºç»“æœé¢„è§ˆ
print("\nè½¬æ¢ç»“æœé¢„è§ˆ:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 8. ä¿å­˜å¹¶ä¸‹è½½ï¼ˆå®Œå…¨å…¼å®¹Excelï¼‰
output_file = "output_split_datetime.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- å®Œæ•´ä»£ç ç»“æŸ ---

#å¤šæ—¶åŒºå›½å®¶è½¬æ¢æˆlocaltimeå–äººå£æœ€å¤šçš„æ—¶åŒº
# å®‰è£…ä¾èµ–åŒ…ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
!pip install pytz pandas

# --- å®Œæ•´ä»£ç å¼€å§‹ ---
import pandas as pd
import pytz
from google.colab import files

# 1. ä¸Šä¼ Excelæ–‡ä»¶ï¼ˆColabäº¤äº’å¼å¼¹çª—ï¼‰
uploaded = files.upload()
file_name = next(iter(uploaded))  # è·å–ä¸Šä¼ æ–‡ä»¶å

# 2. è¯»å–Excelæ•°æ®
df = pd.read_excel(file_name, engine='openpyxl')

# 3. è§£æUTCæ—¶é—´åˆ—ä¸ºæ—¶åŒºæ•æ„Ÿå¯¹è±¡
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. å®šä¹‰å¤šæ—¶åŒºå›½å®¶çš„ä¸»æ—¶åŒºæ˜ å°„è¡¨ï¼ˆäººå£æœ€å¤š/æœ€å¸¸ç”¨ï¼‰
MAIN_TIMEZONES = {
    # å›½å®¶ä»£ç  : æŒ‡å®šæ—¶åŒº
    "US": "America/New_York",     # ç¾å›½ä¸œéƒ¨ï¼ˆè¦†ç›–44%äººå£ï¼‰
    "CA": "America/Toronto",      # åŠ æ‹¿å¤§ä¸œéƒ¨ï¼ˆå®‰å¤§ç•¥çœï¼‰
    "BR": "America/Sao_Paulo",    # å·´è¥¿ä¸œå—éƒ¨ï¼ˆåœ£ä¿ç½—ï¼‰
    "IN": "Asia/Kolkata",         # å°åº¦æ ‡å‡†æ—¶é—´ï¼ˆISTï¼‰
    "RU": "Europe/Moscow",        # ä¿„ç½—æ–¯è¥¿éƒ¨ï¼ˆè«æ–¯ç§‘ï¼‰
    "MX": "America/Mexico_City"   # å¢¨è¥¿å“¥ä¸­éƒ¨
}

def get_timezone(region_code):
    try:
        # ä¼˜å…ˆä½¿ç”¨é¢„å®šä¹‰çš„ä¸»æ—¶åŒº
        if region_code in MAIN_TIMEZONES:
            return pytz.timezone(MAIN_TIMEZONES[region_code])
        # å…¶ä»–å›½å®¶é€‰æ‹©ç¬¬ä¸€ä¸ªæ—¶åŒº
        return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°å›½å®¶ä»£ç  '{region_code}' çš„æ—¶åŒº")
        return None

# 5. æ—¶é—´è½¬æ¢ä¸åˆ†å‰²ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
def convert_and_split_time(row):
    tz = get_timezone(row["region"])
    if not tz:
        return None, None

    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆä¿ç•™ç§’ï¼‰
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # åˆ†å‰²ä¸ºæ—¥æœŸå’Œæ—¶é—´å­—ç¬¦ä¸²
    return (
        local_time.date().isoformat(),         # localdateåˆ—ï¼šYYYY-MM-DD
        local_time.time().strftime("%H:%M:%S") # localtimeåˆ—ï¼šHH:MM:SS
    )

# ç”Ÿæˆä¸¤åˆ—
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 6. æ¸…ç†åŸå§‹UTCæ—¶é—´åˆ—çš„æ—¶åŒºä¿¡æ¯ï¼ˆç¡®ä¿Excelå…¼å®¹ï¼‰
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 7. æ˜¾ç¤ºç»“æœé¢„è§ˆ
print("\nè½¬æ¢ç»“æœé¢„è§ˆ:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 8. ä¿å­˜å¹¶ä¸‹è½½
output_file = "output_multicountry_time.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- å®Œæ•´ä»£ç ç»“æŸ ---

#å¢¨è¥¿å“¥2022.10.30å–æ¶ˆå¤ä»¤æ—¶ï¼Œæ—¶é—´æ£€æŸ¥
# å®‰è£…ä¾èµ–åŒ…ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
!pip install pytz pandas

# --- å®Œæ•´ä»£ç å¼€å§‹ ---
import pandas as pd
import pytz
from datetime import datetime
from google.colab import files

# 1. ä¸Šä¼ Excelæ–‡ä»¶ï¼ˆColabäº¤äº’å¼å¼¹çª—ï¼‰
uploaded = files.upload()
file_name = next(iter(uploaded))  # è·å–ä¸Šä¼ æ–‡ä»¶å

# 2. è¯»å–Excelæ•°æ®
df = pd.read_excel(file_name)

# 3. è§£æUTCæ—¶é—´åˆ—ä¸ºæ—¶åŒºæ•æ„Ÿå¯¹è±¡
df["publishedAt"] = pd.to_datetime(df["publishedAt"], utc=True)

# 4. å®šä¹‰å¤šæ—¶åŒºå›½å®¶çš„ä¸»æ—¶åŒºæ˜ å°„è¡¨
MAIN_TIMEZONES = {
    "US": "America/New_York",
    "CA": "America/Toronto",
    "BR": "America/Sao_Paulo",
    "IN": "Asia/Kolkata",
    "RU": "Europe/Moscow",
    "MX": "America/Mexico_City"
}

# 5. å¢¨è¥¿å“¥æ—¶åŒºç‰¹æ®Šå¤„ç†å‡½æ•°ï¼ˆä¿®å¤ç‚¹1ï¼šæ­£ç¡®åˆå§‹åŒ–åˆ†ç•Œæ—¶é—´ï¼‰
def get_mexico_timezone(utc_time):
    # å¢¨è¥¿å“¥æ”¿ç­–å˜æ›´åˆ†ç•Œç‚¹ï¼ˆUTCæ—¶é—´ï¼‰
    cutoff_date = pd.Timestamp("2022-10-30T00:00:00Z").tz_convert("UTC")  # ä¿®å¤æ‹¼å†™å’Œæ—¶åŒºæ–¹æ³•

    if utc_time <= cutoff_date:
        return pytz.timezone("America/Mexico_City")
    else:
        # åˆ›å»ºå›ºå®šUTC-6çš„æ—¶åŒºï¼ˆæ— å¤ä»¤æ—¶ï¼‰
        return pytz.FixedOffset(-6*60)  # UTC-6

# 6. åŠ¨æ€è·å–æ—¶åŒºçš„å‡½æ•°ï¼ˆä¿®å¤ç‚¹2ï¼šä¼ é€’æ­£ç¡®çš„æ—¶åŒºå‚æ•°ï¼‰
def get_timezone(region_code, utc_time):
    try:
        if region_code == "MX":
            return get_mexico_timezone(utc_time)
        elif region_code in MAIN_TIMEZONES:
            return pytz.timezone(MAIN_TIMEZONES[region_code])
        else:
            return pytz.timezone(pytz.country_timezones[region_code][0])
    except (KeyError, IndexError):
        print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°å›½å®¶ä»£ç  '{region_code}' çš„æ—¶åŒº")
        return None

# 7. æ—¶é—´è½¬æ¢ä¸åˆ†å‰²ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
def convert_and_split_time(row):
    tz = get_timezone(row["region"], row["publishedAt"])  # ä¿®å¤ç‚¹3ï¼šç¡®ä¿ä¼ å…¥æ—¶åŒºæ•æ„Ÿå¯¹è±¡
    if not tz:
        return None, None

    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´å¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
    local_time = row["publishedAt"].astimezone(tz).replace(tzinfo=None)

    # åˆ†å‰²ä¸ºæ—¥æœŸå’Œæ—¶é—´å­—ç¬¦ä¸²
    return (
        local_time.date().isoformat(),
        local_time.time().strftime("%H:%M:%S")
    )

# åº”ç”¨è½¬æ¢å‡½æ•°ç”Ÿæˆä¸¤åˆ—
df[["localdate", "localtime"]] = df.apply(
    lambda row: convert_and_split_time(row),
    axis=1,
    result_type="expand"
)

# 8. æ¸…ç†åŸå§‹UTCæ—¶é—´åˆ—çš„æ—¶åŒºä¿¡æ¯
df["publishedAt"] = df["publishedAt"].dt.tz_localize(None)

# 9. æ˜¾ç¤ºç»“æœé¢„è§ˆ
print("\nè½¬æ¢ç»“æœé¢„è§ˆ:")
print(df[["publishedAt", "region", "localdate", "localtime"]].head())

# 10. ä¿å­˜å¹¶ä¸‹è½½
output_file = "output_timezone_fixed.xlsx"
df.to_excel(output_file, index=False)
files.download(output_file)
# --- å®Œæ•´ä»£ç ç»“æŸ ---

#æ£€æŸ¥ä¸¤ä»½MXçš„å†…å®¹æ˜¯å¦æœ‰å·®å¼‚
# å®‰è£…ä¾èµ–åŒ…ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
!pip install pandas

# --- å®Œæ•´ä»£ç å¼€å§‹ ---
import pandas as pd
from google.colab import files

def compare_excel_files():
    # 1. ä¸Šä¼ ç¬¬ä¸€ä¸ªæ–‡ä»¶
    print("è¯·ä¸Šä¼ ç¬¬ä¸€ä¸ªExcelæ–‡ä»¶ï¼š")
    uploaded_file1 = files.upload()
    file1_name = next(iter(uploaded_file1))

    # 2. ä¸Šä¼ ç¬¬äºŒä¸ªæ–‡ä»¶
    print("\nè¯·ä¸Šä¼ ç¬¬äºŒä¸ªExcelæ–‡ä»¶ï¼š")
    uploaded_file2 = files.upload()
    file2_name = next(iter(uploaded_file2))

    # 3. è¯»å–ä¸¤ä¸ªæ–‡ä»¶
    try:
        df1 = pd.read_excel(file1_name)
        df2 = pd.read_excel(file2_name)
    except Exception as e:
        print(f"\né”™è¯¯: æ–‡ä»¶è¯»å–å¤±è´¥ - {str(e)}")
        return

    # 4. æ‰§è¡Œæ·±åº¦å¯¹æ¯”
    print("\nå¯¹æ¯”ç»“æœï¼š")
    if df1.shape != df2.shape:
        print("âŒ æ–‡ä»¶ç»“æ„ä¸åŒ (è¡Œåˆ—æ•°ä¸ä¸€è‡´)")
        print(f"æ–‡ä»¶1: {df1.shape[0]}è¡Œ{df1.shape[1]}åˆ—")
        print(f"æ–‡ä»¶2: {df2.shape[0]}è¡Œ{df2.shape[1]}åˆ—")
        return

    # æ£€æŸ¥åˆ—åæ˜¯å¦ä¸€è‡´
    if list(df1.columns) != list(df2.columns):
        print("âŒ åˆ—å/åˆ—é¡ºåºä¸ä¸€è‡´")
        print("æ–‡ä»¶1åˆ—å:", list(df1.columns))
        print("æ–‡ä»¶2åˆ—å:", list(df2.columns))
        return

    # æ£€æŸ¥æ•°æ®å†…å®¹
    comparison = df1.compare(df2)
    if comparison.empty:
        print("âœ… æ–‡ä»¶å†…å®¹å®Œå…¨ä¸€è‡´")
    else:
        print(f"âŒ å‘ç° {len(comparison)} å¤„å·®å¼‚")
        print("\nå·®å¼‚è¯¦æƒ…ï¼š")
        print(comparison)

# æ‰§è¡Œå¯¹æ¯”
compare_excel_files()
# --- å®Œæ•´ä»£ç ç»“æŸ ---
#è¿˜æ˜¯ç”¨å›ä¹‹å‰è½¬æ¢çš„localtimeï¼Œä¸è¦ç”¨è¿™ä»½æ–°ä»£ç æ¥è½¬åŒ–äº†ã€‚åº”è¯¥æ˜¯åœ¨10.29å’Œ10.30æœ‰ä¸€ä¸ªè°ƒæ•´ç•Œé™ï¼Œæ‰‹åŠ¨æ•´ç†åè€Œä¼šå‡ºé”™

#åˆ—åä¿®æ”¹å’Œé‡æ–°æ’åº
import pandas as pd
from google.colab import files

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
file_name = next(iter(uploaded))

# è¯»å–Excelæ–‡ä»¶
df = pd.read_excel(file_name)

# åˆ—é‡å‘½å
df.rename(columns={
    df.columns[10]: 'Time_to_trend(Hours)',  # Kåˆ—
    df.columns[13]: 'Local_Publishdate',     # Nåˆ—
    df.columns[11]: 'Trending_time_bucket',  # Låˆ—
    df.columns[14]: 'Local_publishtime',     # Oåˆ—
    df.columns[12]: 'Same_video_group'       # Måˆ—
}, inplace=True)

# å®šä¹‰æ–°åˆ—é¡ºåºï¼ˆä¿æŒåŸå§‹é€»è¾‘ï¼‰
new_order = [9, 12, 13, 14, 5, 6, 7, 8, 3, 11, 10, 1, 4, 0, 2]
df = df.iloc[:, new_order]

# ä¿å­˜å¹¶ä¸‹è½½æ–°æ–‡ä»¶
output_filename = 'processed_file.xlsx'
df.to_excel(output_filename, index=False)
files.download(output_filename)

print("âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼å·²è‡ªåŠ¨ä¸‹è½½æ–°æ–‡ä»¶")

#å»é™¤engagementä¸­å•åˆ—0æœ€å¤šçš„
import pandas as pd
from google.colab import files

# ä¸Šä¼ Excelæ–‡ä»¶
uploaded = files.upload()

# è¯»å–Excelæ•°æ®
file_name = next(iter(uploaded))
df = pd.read_excel(file_name)

# è®¡ç®—æ¯è¡Œå•ä¸ª0çš„æ•°é‡
df['zero_count'] = (df[['view_count', 'likes', 'dislikes', 'comment_count']] == 0).sum(axis=1)

# å®šä¹‰åˆ†ç»„å¤„ç†å‡½æ•°
def process_group(group):
    if group['zero_count'].nunique() > 1:  # å¦‚æœå­˜åœ¨å·®å¼‚
        min_count = group['zero_count'].min()
        return group[group['zero_count'] == min_count]
    return group

# æŒ‰Båˆ—åˆ†ç»„å¤„ç†
result_df = df.groupby('Same_video_group', group_keys=False).apply(process_group).drop(columns=['zero_count'])

# ä¿å­˜ç»“æœ
result_df.to_excel('processed_result.xlsx', index=False)
files.download('processed_result.xlsx')

#å°†publishatï¼Œtrendingdateï¼Œvideoidå’Œchannelidå…¨éƒ¨æ¸…é™¤
!pip install openpyxl xlrd==1.2.0

import pandas as pd
from google.colab import files
import os
import shutil

# äº¤äº’å¼ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()

# å¤„ç†æ¯ä¸ªæ–‡ä»¶
for filename in uploaded.keys():
    try:
        # ç¡®å®šæ–‡ä»¶ç±»å‹å’Œè¯»å–å¼•æ“
        if filename.lower().endswith('.xlsx'):
            engine = 'openpyxl'
        elif filename.lower().endswith('.xls'):
            engine = 'xlrd'
        else:
            print(f"è·³è¿‡éExcelæ–‡ä»¶: {filename}")
            continue

        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(filename, engine=engine)

        # åˆ é™¤L/M/N/Oåˆ—ï¼ˆç¬¬12-15åˆ—ï¼Œå¯¹åº”ç´¢å¼•11-14ï¼‰
        if len(df.columns) >= 15:
            cols_to_drop = df.columns[11:15]
        else:
            cols_to_drop = df.columns[11:len(df.columns)]

        df.drop(columns=cols_to_drop, inplace=True)

        # åˆ›å»ºæ–°æ–‡ä»¶åå¹¶ä¿å­˜
        base_name = os.path.splitext(filename)[0]
        new_filename = f'useful_{base_name}.xlsx'
        df.to_excel(new_filename, index=False, engine='openpyxl')
        print(f"æ–‡ä»¶ {filename} å¤„ç†å®Œæˆï¼Œä¿å­˜ä¸º {new_filename}")

    except Exception as e:
        print(f"å¤„ç† {filename} æ—¶å‡ºé”™: {str(e)}")

# æ‰“åŒ…å¤„ç†åçš„æ–‡ä»¶
os.makedirs('processed_files', exist_ok=True)
for file in os.listdir():
    if file.startswith('useful_') and file.endswith('.xlsx'):
        shutil.move(file, os.path.join('processed_files', file))

shutil.make_archive('processed_files', 'zip', 'processed_files')

# ä¸‹è½½å‹ç¼©åŒ…
print("\næ­£åœ¨å‡†å¤‡ä¸‹è½½é“¾æ¥...")
files.download('processed_files.zip')

#è‹¥trending_time_bucketä¸­æœ‰ç›¸åŒçš„ï¼Œåˆ™åªä¿ç•™è¡Œæ•°æœ€å°çš„ï¼ˆæ—¶é—´æ›´è¿‘çš„ï¼‰è¡Œ
import pandas as pd
import numpy as np
from google.colab import files

# è®©ç”¨æˆ·ä¸Šä¼  Excel æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
required_columns = ['Same_video_group', 'Trending_time_bucket']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")

# æ·»åŠ ä¸€ä¸ªç´¢å¼•åˆ—æ¥ä¿ç•™åŸå§‹é¡ºåº
df['Row_Index'] = np.arange(len(df))

# æŒ‰ 'Same_video_group' å’Œ 'Trending_time_bucket' è¿›è¡Œåˆ†ç»„ï¼Œä¿ç•™æ¯ç»„çš„ç¬¬ä¸€è¡Œ
filtered_df = df.sort_values(by=['Same_video_group', 'Trending_time_bucket', 'Row_Index'])
filtered_df = filtered_df.groupby(['Same_video_group', 'Trending_time_bucket'], as_index=False).first()

# åˆ é™¤è¾…åŠ©åˆ—
filtered_df = filtered_df.drop(columns=['Row_Index'])

# ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼è¯·ä¸‹è½½æ–‡ä»¶ã€‚")

import pandas as pd
import numpy as np
from google.colab import files

# è®©ç”¨æˆ·ä¸Šä¼  Excel æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
required_columns = ['Same_video_group', 'Trending_time_bucket']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")

# æ·»åŠ ä¸€ä¸ªç´¢å¼•åˆ—æ¥ä¿ç•™åŸå§‹é¡ºåº
df['Row_Index'] = np.arange(len(df))

# æŒ‰ 'Same_video_group' å’Œ 'Trending_time_bucket' è¿›è¡Œåˆ†ç»„ï¼Œä¿ç•™æ¯ç»„çš„ç¬¬ä¸€è¡Œ
filtered_indices = df.groupby(['Same_video_group', 'Trending_time_bucket'])['Row_Index'].idxmin()
filtered_df = df.loc[filtered_indices].sort_values(by='Row_Index').drop(columns=['Row_Index'])

# ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼è¯·ä¸‹è½½æ–‡ä»¶ã€‚")

#ä¸¤å°æ—¶åˆ’åˆ†æ—¶é—´ç»„ï¼ŒåŒ…å‰ä¸åŒ…å
import pandas as pd
import numpy as np
from google.colab import files

# è®©ç”¨æˆ·ä¸Šä¼  Excel æ–‡ä»¶
uploaded = files.upload()

# è·å–ä¸Šä¼ çš„æ–‡ä»¶å
filename = list(uploaded.keys())[0]

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel(filename)

# ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
required_columns = ['Same_video_group', 'Trending_time_bucket', 'Local_publishtime']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"Excel æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")

# æ·»åŠ ä¸€ä¸ªç´¢å¼•åˆ—æ¥ä¿ç•™åŸå§‹é¡ºåº
df['Row_Index'] = np.arange(len(df))

# è§£æ 'Local_publishtime' å¹¶è®¡ç®—æ‰€å±æ—¶é—´ç»„
def get_time_group(time_str):
    hour = int(time_str.split(':')[0])  # æå–å°æ—¶éƒ¨åˆ†
    return (hour // 2) + 1  # è®¡ç®—æ‰€å±æ—¶é—´æ®µ

df['Time_Group'] = df['Local_publishtime'].astype(str).apply(get_time_group)

# æŒ‰ 'Same_video_group' å’Œ 'Trending_time_bucket' è¿›è¡Œåˆ†ç»„ï¼Œä¿ç•™æ¯ç»„çš„ç¬¬ä¸€è¡Œ
filtered_indices = df.groupby(['Same_video_group', 'Trending_time_bucket'])['Row_Index'].idxmin()
filtered_df = df.loc[filtered_indices].sort_values(by='Row_Index').drop(columns=['Row_Index'])

# ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
output_filename = "filtered_data.xlsx"
filtered_df.to_excel(output_filename, index=False)

# æä¾›ä¸‹è½½é“¾æ¥
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼è¯·ä¸‹è½½æ–‡ä»¶ã€‚")

#çœ‹trendingtimebucketå’Œtimegroupçš„åˆ†å¸ƒæƒ…å†µ
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# æ–‡ä»¶ä¸Šä¼ 
uploaded = files.upload()
file_name = next(iter(uploaded.keys()))
df = pd.read_excel(file_name)

# å®šä¹‰é¡ºåºå¸¸é‡
TREND_ORDER = ['12h', '12-24h', '24-48h', '48+h']
TIME_GROUP_ORDER = list(range(1, 13))

# åŸå§‹æ•°æ®ç»Ÿè®¡
print("ã€åŸå§‹æ•°æ®ç»Ÿè®¡ã€‘")
print("\nJåˆ—ï¼ˆTrending_time_bucketï¼‰åˆ†å¸ƒï¼š")
trend_counts = df['Trending_time_bucket'].value_counts().reindex(TREND_ORDER, fill_value=0)
print(trend_counts)

print("\nLåˆ—ï¼ˆTime_Groupï¼‰åˆ†å¸ƒï¼š")
time_group_counts = df['Time_Group'].value_counts().reindex(TIME_GROUP_ORDER, fill_value=0)
print(time_group_counts)

# åŸå§‹æ•°æ®å¯è§†åŒ–
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='Trending_time_bucket', order=TREND_ORDER)
plt.title('Trending_time_bucket åˆ†å¸ƒï¼ˆåŸå§‹æ•°æ®ï¼‰')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='Time_Group', order=TIME_GROUP_ORDER)
plt.title('Time_Group åˆ†å¸ƒï¼ˆåŸå§‹æ•°æ®ï¼‰')
plt.xticks(ticks=range(12), labels=TIME_GROUP_ORDER)
plt.show()

# æŒ‰Same_video_groupå»é‡åç»Ÿè®¡
unique_df = df.drop_duplicates(subset=['Same_video_group'])

print("\n\nã€æŒ‰Same_video_groupå»é‡åç»Ÿè®¡ã€‘")
print("\nJåˆ—ï¼ˆTrending_time_bucketï¼‰åˆ†å¸ƒï¼š")
unique_trend_counts = unique_df['Trending_time_bucket'].value_counts().reindex(TREND_ORDER, fill_value=0)
print(unique_trend_counts)

print("\nLåˆ—ï¼ˆTime_Groupï¼‰åˆ†å¸ƒï¼š")
unique_time_group_counts = unique_df['Time_Group'].value_counts().reindex(TIME_GROUP_ORDER, fill_value=0)
print(unique_time_group_counts)

#æ•´ç†å‘ç°12hå¤ªå°‘ï¼Œ48+hå¤ªå¤šäº†
#å¦‚æœåŒæ—¶æœ‰48+hå’Œ12hçš„ï¼Œå°±å°†48+hçš„åˆ æ‰
#å¦‚æœåŒæ—¶å­˜åœ¨â€˜24-48hâ€™æˆ–è€…'12-24h'å’Œâ€˜48+hâ€™ï¼Œé‚£ä¹ˆä¹Ÿå°†â€˜48+hâ€™çš„æ•´è¡Œåˆ æ‰
from google.colab import files
import pandas as pd

def process_data(df):
    # å®šä¹‰éœ€è¦æ£€æµ‹çš„å†²çªç»„åˆ
    CONFLICT_CONDITIONS = [
        {'12h', '48+h'},  # åŸå§‹å†²çªæ¡ä»¶
        {'24-48h', '48+h'},  # æ–°å¢å†²çªæ¡ä»¶
    {'12-24h', '48+h'}  # å‡è®¾æ–°å¢çš„ç¬¬ä¸‰ç§å†²çªæ¡ä»¶
    ]

    final_dfs = []

    # æŒ‰è§†é¢‘ç»„åˆ†ç»„å¤„ç†
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä»»ä¸€å†²çªæ¡ä»¶
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # å¤„ç†å†²çª
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '48+h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†çš„äº¤äº’æµç¨‹
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# æ‰§è¡Œå¤„ç†
processed_df = process_data(original_df)

# ç”Ÿæˆç»“æœæ–‡ä»¶
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# ä¸‹è½½æ–‡ä»¶
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼")
print(f"åŸå§‹æ•°æ®é‡: {len(original_df)} è¡Œ")
print(f"å¤„ç†åæ•°æ®é‡: {len(processed_df)} è¡Œ")
print(f"åˆ é™¤è®°å½•æ•°: {len(original_df) - len(processed_df)} è¡Œ")
print("å·²ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼š" + output_filename)

#è‹¥12hå’Œ24-48åŒæ—¶å­˜åœ¨ï¼Œåˆ™åˆ 24-48
from google.colab import files
import pandas as pd

def process_data(df):
    # å®šä¹‰éœ€è¦æ£€æµ‹çš„å†²çªç»„åˆ
    CONFLICT_CONDITIONS = [ {'12h', '24-48h'}]

    final_dfs = []

    # æŒ‰è§†é¢‘ç»„åˆ†ç»„å¤„ç†
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä»»ä¸€å†²çªæ¡ä»¶
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # å¤„ç†å†²çª
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '24-48h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†çš„äº¤äº’æµç¨‹
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# æ‰§è¡Œå¤„ç†
processed_df = process_data(original_df)

# ç”Ÿæˆç»“æœæ–‡ä»¶
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# ä¸‹è½½æ–‡ä»¶
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼")
print(f"åŸå§‹æ•°æ®é‡: {len(original_df)} è¡Œ")
print(f"å¤„ç†åæ•°æ®é‡: {len(processed_df)} è¡Œ")
print(f"åˆ é™¤è®°å½•æ•°: {len(original_df) - len(processed_df)} è¡Œ")
print("å·²ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼š" + output_filename)

#è‹¥12-24ä¸24-48åŒåœ¨ï¼Œåˆ 24-48ï¼ŒRUå’ŒINä¸å‚ä¸
from google.colab import files
import pandas as pd

def process_data(df):
    # å®šä¹‰éœ€è¦æ£€æµ‹çš„å†²çªç»„åˆ
    CONFLICT_CONDITIONS = [ {'12-24h', '24-48h'}]

    final_dfs = []

    # æŒ‰è§†é¢‘ç»„åˆ†ç»„å¤„ç†
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä»»ä¸€å†²çªæ¡ä»¶
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # å¤„ç†å†²çª
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '24-48h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†çš„äº¤äº’æµç¨‹
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# æ‰§è¡Œå¤„ç†
processed_df = process_data(original_df)

# ç”Ÿæˆç»“æœæ–‡ä»¶
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# ä¸‹è½½æ–‡ä»¶
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼")
print(f"åŸå§‹æ•°æ®é‡: {len(original_df)} è¡Œ")
print(f"å¤„ç†åæ•°æ®é‡: {len(processed_df)} è¡Œ")
print(f"åˆ é™¤è®°å½•æ•°: {len(original_df) - len(processed_df)} è¡Œ")
print("å·²ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼š" + output_filename)

#MXå’ŒKRçš„12-24åˆ†ç»™12
from google.colab import files
import pandas as pd

def process_data(df):
    # å®šä¹‰éœ€è¦æ£€æµ‹çš„å†²çªç»„åˆ
    CONFLICT_CONDITIONS = [ {'12-24h', '12h'}]

    final_dfs = []

    # æŒ‰è§†é¢‘ç»„åˆ†ç»„å¤„ç†
    for group_id, group_df in df.groupby('Same_video_group'):
        existing_values = set(group_df['Trending_time_bucket'])

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä»»ä¸€å†²çªæ¡ä»¶
        conflict_detected = any(
            condition.issubset(existing_values)
            for condition in CONFLICT_CONDITIONS
        )

        # å¤„ç†å†²çª
        if conflict_detected:
            filtered_group = group_df[group_df['Trending_time_bucket'] != '12-24h']
            final_dfs.append(filtered_group)
        else:
            final_dfs.append(group_df)

    return pd.concat(final_dfs)

# æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†çš„äº¤äº’æµç¨‹
uploaded = files.upload()
filename = next(iter(uploaded.keys()))
original_df = pd.read_excel(filename)

# æ‰§è¡Œå¤„ç†
processed_df = process_data(original_df)

# ç”Ÿæˆç»“æœæ–‡ä»¶
output_filename = f'processed_v2_{filename}'
processed_df.to_excel(output_filename, index=False)

# ä¸‹è½½æ–‡ä»¶
files.download(output_filename)

print("å¤„ç†å®Œæˆï¼")
print(f"åŸå§‹æ•°æ®é‡: {len(original_df)} è¡Œ")
print(f"å¤„ç†åæ•°æ®é‡: {len(processed_df)} è¡Œ")
print(f"åˆ é™¤è®°å½•æ•°: {len(original_df) - len(processed_df)} è¡Œ")
print("å·²ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼š" + output_filename)

#å°†æ‰€æœ‰å¤„ç†å¥½çš„è¡¨æ ¼ä¸²æˆä¸€å¼ è¡¨
from google.colab import files
import pandas as pd
from io import BytesIO
import ipywidgets as widgets
from IPython.display import display, clear_output
import time
import os

def merge_files():
    # åˆ›å»ºæ”¯æŒå¤šæ ¼å¼çš„ä¸Šä¼ æ§ä»¶
    upload = widgets.FileUpload(
        accept='.xlsx, .xls, .csv',  # æ·»åŠ CSVæ”¯æŒ
        multiple=True,
        description='é€‰æ‹©æ–‡ä»¶'
    )

    # åˆ›å»ºæ ¼å¼è¯´æ˜æ ‡ç­¾
    format_info = widgets.HTML(
        value="<i>æ”¯æŒæ ¼å¼ï¼šExcel(.xlsx/.xls) å’Œ CSV(.csv)</i>"
    )

    display(format_info, upload)

    out = widgets.Output()
    display(out)

    with out:
        print("è¯·é€šè¿‡ä¸Šæ–¹æŒ‰é’®ä¸Šä¼ æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰...")
        while len(upload.value) == 0:
            time.sleep(1)

    try:
        uploaded_files = upload.value
        merged_df = pd.DataFrame()

        with out:
            clear_output()
            print(f"æ”¶åˆ° {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

            for i, (filename, file_data) in enumerate(uploaded_files.items()):
                # è‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹
                ext = os.path.splitext(filename)[1].lower()

                # è¿›åº¦æ¡
                progress = widgets.FloatProgress(
                    value=(i+1)/len(uploaded_files)*100,
                    description='å¤„ç†è¿›åº¦:',
                    bar_style='info',
                    orientation='horizontal'
                )
                display(progress)

                # æ ¹æ®æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
                try:
                    if ext in ['.xlsx', '.xls']:
                        df = pd.read_excel(BytesIO(file_data))
                    elif ext == '.csv':
                        # å°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                        try:
                            df = pd.read_csv(BytesIO(file_data))
                        except UnicodeDecodeError:
                            df = pd.read_csv(BytesIO(file_data), encoding='gbk')
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {ext}")
                except Exception as e:
                    print(f"æ–‡ä»¶ {filename} è¯»å–å¤±è´¥: {str(e)}")
                    continue

                # åˆå¹¶æ•°æ®
                merged_df = pd.concat([merged_df, df], axis=0, ignore_index=True, join='outer')

                print(f"å·²å¤„ç†: {filename} ({df.shape[0]}è¡Œ)")
                progress.close()

            # ç»“æœå±•ç¤º
            print("\nåˆå¹¶å®Œæˆï¼æ•°æ®é›†æ‘˜è¦ï¼š")
            print(merged_df.describe(include='all'))

            # ä¿å­˜åˆå¹¶ç»“æœ
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            output_file = f"merged_data_{timestamp}.xlsx"
            merged_df.to_excel(output_file, index=False)

            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            download_btn = widgets.Button(description="ä¸‹è½½åˆå¹¶æ–‡ä»¶")
            def on_download_click(b):
                files.download(output_file)
            download_btn.on_click(on_download_click)

            display(download_btn)

    except Exception as e:
        with out:
            print(f"å¤„ç†å¤±è´¥: {str(e)}")
        raise

merge_files()

from google.colab import files
import pandas as pd
from io import BytesIO
import os
import time

def merge_files_safely():
    print("ğŸ“‚ è¯·é€‰æ‹©è¦åˆå¹¶çš„æ–‡ä»¶ï¼ˆæ”¯æŒ .xlsx / .xls / .csvï¼‰ï¼š")
    uploaded = files.upload()

    merged = pd.DataFrame()

    for name, file in uploaded.items():
        ext = os.path.splitext(name)[1].lower()
        try:
            if ext in ['.xlsx', '.xls']:
                df = pd.read_excel(BytesIO(file))
            elif ext == '.csv':
                try:
                    df = pd.read_csv(BytesIO(file))
                except:
                    df = pd.read_csv(BytesIO(file), encoding='gbk')
            else:
                print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ï¼š{name}")
                continue

            merged = pd.concat([merged, df], ignore_index=True)
            print(f"âœ… å·²åˆå¹¶ï¼š{name}ï¼ˆ{len(df)} è¡Œï¼‰")
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥ï¼š{name} - {e}")

    if merged.empty:
        print("âš ï¸ æ²¡æœ‰æˆåŠŸåˆå¹¶ä»»ä½•æ–‡ä»¶ã€‚")
        return

    filename = f"åˆå¹¶ç»“æœ_{time.strftime('%Y%m%d_%H%M%S')}.xlsx"
    merged.to_excel(filename, index=False)
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼å…± {len(merged)} è¡Œï¼Œ{len(merged.columns)} åˆ—")
    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½åˆå¹¶ç»“æœæ–‡ä»¶...")
    files.download(filename)

merge_files_safely()

#ç»™'Trending_time_bucket'åˆ†ç»„
import pandas as pd
from google.colab import files
from io import BytesIO

# 1. è®©ç”¨æˆ·ä¸Šä¼  Excel æ–‡ä»¶
uploaded = files.upload()

# 2. è¯»å–ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆåªå–ç¬¬ä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶ï¼‰
for filename in uploaded:
    file = BytesIO(uploaded[filename])
    break

# 3. è¯»å– Excel æ–‡ä»¶åˆ° DataFrame
df = pd.read_excel(file)

# 4. å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ ¹æ® 'Trending_time_bucket' åˆ†ç»„
def assign_bucket(value):
    try:
        value = str(value).lower().strip().replace('h', '').replace('+', '')
        if '-' in value:
            start, end = map(float, value.split('-'))
        else:
            start = float(value)
            end = start

        if end <= 12:
            return 'bucket1'
        elif 12 < end <= 24:
            return 'bucket2'
        elif 24 < end < 48:
            return 'bucket3'
        elif value = '48'
            return 'bucket4'
    except:
        return 'unknown'

# 5. åº”ç”¨å‡½æ•°å¹¶æ–°å»ºä¸€åˆ—
df['bucket_group'] = df['Trending_time_bucket'].apply(assign_bucket)

# 6. ä¿å­˜ä¸ºæ–°çš„Excelæ–‡ä»¶
output_filename = 'processed_file.xlsx'
df.to_excel(output_filename, index=False)

# 7. è‡ªåŠ¨ä¸‹è½½
files.download(output_filename)