# -*- coding: utf-8 -*-
"""Kruskal-Wallis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pois3Dlr70GuKfDmVCAYrCHNG0xFV-Ri
"""

!pip install scikit-posthocs statsmodels
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import scikit_posthocs as sp
from google.colab import files
from statsmodels.stats.multitest import multipletests

# 1. 数据加载与预处理
# ======================
uploaded = files.upload()
file_name = next(iter(uploaded))
df = pd.read_excel(file_name)

# 数据清洗
print(f"原始数据量：{len(df)}")
df = df[df['comment_count'] > 0]
df = df[df['categoryId']!= 24]
print(f"过滤后数据量：{len(df)}")

# 2. Benjamini-Hochberg校正函数
# ======================
def benjamini_hochberg_correction(p_values):
    """对p值进行BH校正，返回q值"""
    _, q_values, _, _ = multipletests(p_values, method='fdr_bh')
    return q_values

# 3. Kruskal-Wallis检验函数（改进抽样版本）
# =============================================
def kruskal_wallis_analysis(data, bucket,
                           sampling_strategy='median',  # 'min'/'median'/'mean'/'custom'
                           min_samples=30,               # 最小样本阈值
                           max_samples=None,             # 最大样本限制
                           balance_ratio=0.8,            # 平衡系数（用于自定义策略）
                           random_state=None,
                           verbose=True):
    """
    改进的抽样策略参数：
    - sampling_strategy: 抽样基准
        'min' : 按最小时间组样本量
        'median' : 按中位数样本量
        'mean' : 按平均样本量
        'custom' : 按平衡系数计算 (balance_ratio * max_size + (1-balance_ratio)*min_size)
    - min_samples: 每组最小样本要求（至少保留的样本量）
    - max_samples: 每组最大样本限制（避免过大样本）
    - balance_ratio: 自定义策略的平衡系数（0-1）
    """
    bucket_data = data[data['bucket_label'] == bucket]
    grouped = bucket_data.groupby('Time_Group')

    if verbose:
        print(f"\n=== Bucket {bucket} Kruskal-Wallis分析 ===")
        print("原始各组样本量：")
        print(grouped.size())

    # 计算各组样本量
    group_sizes = grouped.size()
    valid_groups = group_sizes[group_sizes >= min_samples]

    # 过滤过小组
    if len(valid_groups) < 2:
        print(f"仅有{len(valid_groups)}个组满足最小样本要求（≥{min_samples}）")
        return None

    # 确定抽样基准
    if sampling_strategy == 'min':
        base_size = valid_groups.min()
    elif sampling_strategy == 'median':
        base_size = int(valid_groups.median())
    elif sampling_strategy == 'mean':
        base_size = int(valid_groups.mean())
    elif sampling_strategy == 'custom':
        max_size = valid_groups.max()
        min_size = valid_groups.min()
        base_size = int(balance_ratio * min_size + (1-balance_ratio) * max_size)
    else:
        raise ValueError("不支持的抽样策略")

    # 应用样本量限制
    sample_size = min(base_size, max_samples) if max_samples else base_size
    sample_size = max(sample_size, min_samples)

    if verbose:
        print(f"\n抽样策略：{sampling_strategy}")
        print(f"计算基准量：{base_size} | 最终抽样量：{sample_size}")

    # 分层抽样过程
    balanced_data = pd.DataFrame()
    for name, group in grouped:
        # 过滤过小组
        if len(group) < min_samples:
            continue

        # 动态调整抽样量（保证至少min_samples）
        n_samples = max(min(sample_size, len(group)), min_samples)

        # 执行抽样（样本不足时自动启用有放回抽样）
        replace = n_samples > len(group)
        sampled = group.sample(n=n_samples,
                              replace=replace,
                              random_state=random_state)
        balanced_data = pd.concat([balanced_data, sampled])

    # 准备检验数据
    final_groups = [g['comment_count'].values
                   for _, g in balanced_data.groupby('Time_Group')]

    if verbose:
        print("\n抽样后各组样本量：")
        print(balanced_data.groupby('Time_Group').size())

    # 执行检验
    try:
        h_stat, p_value = stats.kruskal(*final_groups)
    except ValueError as e:
        print(f"检验失败：{str(e)}")
        return None

    # 计算效应量
    n_total = sum(len(g) for g in final_groups)
    epsilon_sq = h_stat / (n_total - 1) if n_total > 1 else 0

    return {
        'H': h_stat,
        'p_raw': p_value,
        'epsilon_sq': epsilon_sq,
        'sampled_groups': len(final_groups),
        'total_samples': n_total,
        'sampling_report': balanced_data.groupby('Time_Group').size().to_dict()
    }

# 4. Dunn事后检验函数
# ======================
def dunn_posthoc(data, bucket, ax=None): # add ax=None
    bucket_data = data[data['bucket_label'] == bucket]

    print(f"\n=== Bucket {bucket} Dunn事后检验 ===")

    if bucket_data['Time_Group'].nunique() < 2:
        print("需要至少2个组进行事后检验")
        return None

    dunn_result = sp.posthoc_dunn(
        bucket_data,
        val_col='comment_count',
        group_col='Time_Group',
        p_adjust='holm'
    )

    # Create figure if ax is not provided
    if ax is None:
        plt.figure(figsize=(8, 6))
        ax = plt.gca()  # Get current axes

    sns.heatmap(dunn_result < 0.05, annot=dunn_result.round(3),
               cmap='Blues', cbar=False, fmt="", ax=ax) # add ax=ax
    ax.set_title(f"Bucket {bucket} - 组间显著性（Holm校正）")
    # plt.show()  # Remove plt.show() here to avoid individual plots

    return dunn_result

# ======================
# 5. Main Execution Flow
# ======================
# First pass: Collect raw p-values
all_p_values = []
kruskal_results = {}

print("\n=== Kruskal-Wallis Initial Results ===")
for bucket in sorted(df['bucket_label'].unique()):
    k_res = kruskal_wallis_analysis(df, bucket)
    if k_res:
        # Store results
        kruskal_results[bucket] = k_res
        all_p_values.append(k_res['p_raw'])

        # Print initial results
        print(f"\nBucket {bucket} Interim Results:")
        print(f"H = {k_res['H']:.2f}")
        print(f"Raw p = {k_res['p_raw']:.3e}")
        print(f"ε² = {k_res['epsilon_sq']:.4f}")

# Perform BH correction
q_values = benjamini_hochberg_correction(all_p_values)

# Add adjusted p-values to results
for i, bucket in enumerate(kruskal_results.keys()):
    kruskal_results[bucket]['p_adj'] = q_values[i]

# Second pass: Final results and post-hoc
print("\n=== Final Results with BH Correction ===")
dunn_results = {}
for bucket in kruskal_results:
    # Get significance level
    p_adj = kruskal_results[bucket]['p_adj']
    sig = '***' if p_adj < 0.001 else '**' if p_adj < 0.01 else '*' if p_adj < 0.05 else 'NS'

    # Print final results
    print(f"\nBucket {bucket}:")
    print(f"H statistic: {kruskal_results[bucket]['H']:.2f}")
    print(f"Raw p-value: {kruskal_results[bucket]['p_raw']:.3e}")
    print(f"Adj p-value: {kruskal_results[bucket]['p_adj']:.3e}")
    print(f"Epsilon-squared: {kruskal_results[bucket]['epsilon_sq']:.4f}")
    print(f"Significance: {sig}")

    # Generate Dunn's plot if significant
    if p_adj < 0.05:
        print(f"\nGenerating Dunn's plot for Bucket {bucket}...")
        plt.close('all')  # Clear previous plots
        d_res = dunn_posthoc(df, bucket)
        dunn_results[bucket] = d_res
        plt.show()  # Force plot display
    else:
        print(f"Skipping Dunn's test for Bucket {bucket} (insignificant)")
# 创建2x2的子图画布
plt.close('all')
fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # 整体画布尺寸
axs = axs.ravel()  # 将二维坐标转换为一维数组
current_ax = 0  # 当前子图位置指针

# Second pass: Final results and post-hoc
print("\n=== Final Results with BH Correction ===")
dunn_results = {}
for bucket in kruskal_results:
    # ... [前面的结果打印代码保持不变] ...

    # Generate Dunn's plot if significant
    if p_adj < 0.05:
        print(f"\nGenerating Dunn's plot for Bucket {bucket}...")

        # 指定子图位置
        d_res = dunn_posthoc(df, bucket, ax=axs[current_ax])  # 需要dunn_posthoc支持ax参数
        dunn_results[bucket] = d_res
        axs[current_ax].set_title(f'Bucket {bucket}')
        current_ax += 1
# ======================
# 6. Enhanced Visualization
# ======================
print("\n=== Final Summary ===")
results_df = pd.DataFrame(kruskal_results).T
results_df['Significance'] = results_df['p_adj'].apply(
    lambda x: '***' if x < 0.001 else '**' if x < 0.01 else '*' if x < 0.05 else 'NS')

# Display formatted results
display(results_df[['H', 'p_raw', 'p_adj', 'epsilon_sq', 'Significance']]
          .rename(columns={
              'H': 'H Statistic',
              'p_raw': 'Raw p-value',
              'p_adj': 'Adj p-value',
              'epsilon_sq': 'Epsilon²',
              'Significance': 'Sig'
          }))

# Effect size visualization
plt.figure(figsize=(10, 6))
sns.barplot(x=results_df.index, y='epsilon_sq', data=results_df)
plt.axhline(0.05, ls='--', c='r', label='Practical threshold (ε²=0.05)')
plt.title('Effect Size Comparison Across Buckets', pad=20)
plt.xlabel('Time Bucket', labelpad=15)
plt.ylabel('Epsilon Squared (ε²)', labelpad=15)
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

ax_effect = axs[current_ax]
sns.barplot(x=results_df.index, y='epsilon_sq', data=results_df, ax=ax_effect)
ax_effect.axhline(0.05, ls='--', c='r', label='Practical threshold (ε²=0.05)')
ax_effect.set_title('Effect Size Comparison')
ax_effect.set_xlabel('Time Bucket', labelpad=10)
ax_effect.set_ylabel('Epsilon Squared (ε²)', labelpad=10)
ax_effect.tick_params(axis='x', rotation=45)
ax_effect.legend()

# 统一调整布局
plt.tight_layout(pad=3.0)  # 增加间距
plt.show()

# 提取Bucket 2的数据
bucket1_data = df[df['bucket_label'] == 2]

# 计算各时间组的view_count中位数
timegroup_median = bucket1_data.groupby('Time_Group')['view_count'].median().sort_values(ascending=False)

print("=== 时间段参与度中位数排名 ===")
print(timegroup_median)

# 统计每个时间段显著优于其他时段的次数
dunn_matrix = dunn_results[2]  # 假设dunn_results已存储Dunn检验结果
win_counts = {}

for time_group in dunn_matrix.index:
    # 计算该时间段显著优于其他组的次数（上三角区域）
    wins = (dunn_matrix.loc[time_group] < 0.05).sum()
    win_counts[time_group] = wins

win_df = pd.Series(win_counts).sort_values(ascending=False)
print("\n=== 时间段显著胜率排名 ===")
print(win_df)

# 计算Cliff's Delta效应量
group3 = df[(df["bucket_label"]==2) & (df["Time_Group"]==3)]["view_count"]
group7 = df[(df["bucket_label"]==2) & (df["Time_Group"]==7)]["view_count"]

delta = (sum(x > y for x in group3 for y in group7) -
         sum(x < y for x in group3 for y in group7)) / (len(group3)*len(group7))

print(f"Time Group 3 vs 7的效应量：{delta:.2f}")

# 绘制凌晨时段的参与度分布
sns.histplot(df[df["Time_Group"] == 3]["view_count"], log_scale=True)
plt.title("Time Group 3参与度分布（对数尺度）")

# 计算各Time Group的样本量
sample_counts = df.groupby(['bucket_label', 'Time_Group']).size().unstack()
print("=== 原始数据样本量分布 ===")
display(sample_counts)

# 找到每个Bucket内最小样本量
min_samples_per_bucket = df.groupby(['bucket_label', 'Time_Group']).size().groupby('bucket_label').min()
print("\n=== 各Bucket最小样本量 ===")
print(min_samples_per_bucket)

import pandas as pd

def balanced_sampling(df, bucket_col='bucket_label', time_group_col='Time_Group'):
    """按Bucket和时间组进行平衡抽样"""
    balanced_dfs = []

    # 对每个Bucket分别处理
    for bucket in df[bucket_col].unique():
        bucket_df = df[df[bucket_col] == bucket]

        # 计算当前Bucket的最小样本量
        min_samples = bucket_df.groupby(time_group_col).size().min()

        # 对每个Time Group抽样
        sampled = bucket_df.groupby(time_group_col).apply(
            lambda x: x.sample(n=min_samples, random_state=42) if len(x) >= min_samples else x
        )
        balanced_dfs.append(sampled)

    return pd.concat(balanced_dfs).reset_index(drop=True)

# 执行抽样
balanced_df = balanced_sampling(df)

# 查看抽样后样本量
balanced_counts = balanced_df.groupby(['bucket_label', 'Time_Group']).size().unstack()
print("\n=== 平衡后样本量分布 ===")
display(balanced_counts)

# 示例：独立分析每个Bucket的Time Group效应
for bucket in balanced_df["bucket_label"].unique():
    print(f"\n=== Bucket {bucket} 独立分析 ===")
    bucket_data = balanced_df[balanced_df["bucket_label"] == bucket]
    result = kruskal_wallis_analysis(bucket_data, bucket)
    if result:
        print(f"H统计量: {result['H']:.2f}")
        print(f"P值: {result['p_raw']:.3e}")
        print(f"ε²: {result['epsilon_sq']:.3f}")

# 示例：提取Bucket 2中位数最高的前3个时段
top3_groups = balanced_df[balanced_df["bucket_label"] == 1].groupby("Time_Group")["view_count"].median().nlargest(3).index.tolist()
print(f"Bucket 2推荐时段：{top3_groups}")

# 对Bucket 2进行Dunn检验
dunn_result = sp.posthoc_dunn(balanced_df[balanced_df["bucket_label"] == 2], val_col='view_count', group_col='Time_Group')
sns.heatmap(dunn_result < 0.05, annot=True, cmap="Blues")

from scipy import stats
import numpy as np

def weighted_kruskal(groups, weights):
    # 合并数据并计算秩次
    combined = np.concatenate(groups)
    ranks = stats.rankdata(combined)

    # 计算每组加权秩次均值
    weighted_rank_means = []
    start = 0
    for i, group in enumerate(groups):
        n = len(group)
        group_ranks = ranks[start:start+n]
        weighted_rank_mean = np.mean(group_ranks) * weights[i]
        weighted_rank_means.append(weighted_rank_mean)
        start += n

    # 计算H统计量
    N = len(combined)
    H = (12 / (N * (N + 1))) * sum(weighted_rank_means) - 3 * (N + 1)

    # 计算自由度并获取p值
    k = len(groups)
    p = stats.chi2.sf(H, df=k-1)
    return H, p

# 示例：假设Bucket 2有12个Time Group，样本量分别为[1000, 2000, ..., 12000]
groups = [df[(df['bucket_label']==2) & (df['Time_Group']==i)]['view_count'].values for i in range(1,13)]
weights = [1.0 / len(g) for g in groups]  # 权重与样本量成反比

H_weighted, p_weighted = weighted_kruskal(groups, weights)
print(f"加权H统计量: {H_weighted:.2f}, P值: {p_weighted:.3e}")

import seaborn as sns

# 计算每个Time Group的样本量和效应量
timegroup_stats = df[df['bucket_label'] == 2].groupby('Time_Group').agg(
    sample_size=('view_count', 'size'),
    median_views=('view_count', 'median')
).reset_index()

# 绘制样本量与中位数关系
sns.scatterplot(data=timegroup_stats, x='sample_size', y='median_views', hue='Time_Group')
plt.title("Bucket 2各时段样本量与参与度中位数关系")
plt.show()

import pandas as pd

# 读取数据（假设数据已加载为 df）
# df = pd.read_csv("your_data.csv")

# 删除 categoryId == 24 的视频
df_filtered = df[df['categoryId'] != 24].copy()
print(f"原始数据量: {len(df)} | 过滤后数据量: {len(df_filtered)}")

# 按 bucket_label 和 Time_Group 分组计算 view_count 中位数
median_results = df_filtered.groupby(['bucket_label', 'Time_Group'])['view_count'].median().reset_index()

# 添加按 bucket_label 分组后，组内 view_count 中位数从大到小排序
median_sorted_by_bucket = median_results.groupby('bucket_label').apply(
    lambda x: x.sort_values('view_count', ascending=False)
).reset_index(drop=True)

# 输出结果
print("\n=== 各分组中位数结果（按Bucket内中位数降序排序） ===")
display(median_sorted_by_bucket)

import seaborn as sns
import matplotlib.pyplot as plt

# 生成透视表
heatmap_data = median_results.pivot(index='bucket_label', columns='Time_Group', values='view_count')

# 绘制热力图
plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_data, annot=True, fmt=".0f", cmap="YlGnBu", linewidths=0.5)
plt.title("各 Bucket 和时间组的观看次数中位数（过滤 categoryId=24 后）")
plt.xlabel("Time Group")
plt.ylabel("Bucket")
plt.show()

# 绘制 Bucket 2 的箱线图（示例）
plt.figure(figsize=(12, 6))
sns.boxplot(
    x='Time_Group',
    y='view_count',
    data=df_filtered[df_filtered['bucket_label'] == 2],
    order=sorted(df_filtered['Time_Group'].unique())
)
plt.yscale("log")  # 对数尺度处理极端值
plt.title("Bucket 2 各时间组观看次数分布（过滤 categoryId=24 后）")
plt.show()